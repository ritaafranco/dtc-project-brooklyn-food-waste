[2022-06-23 10:26:55,509] {processor.py:153} INFO - Started process (PID=47) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:26:55,510] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:26:55,510] {logging_mixin.py:115} INFO - [2022-06-23 10:26:55,510] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:26:55,524] {logging_mixin.py:115} INFO - [2022-06-23 10:26:55,523] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:26:55,524] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:26:55,555] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.049 seconds
[2022-06-23 10:27:25,636] {processor.py:153} INFO - Started process (PID=68) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:27:25,636] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:27:25,637] {logging_mixin.py:115} INFO - [2022-06-23 10:27:25,636] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:27:25,641] {logging_mixin.py:115} INFO - [2022-06-23 10:27:25,641] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:27:25,641] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:27:25,654] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.021 seconds
[2022-06-23 10:27:55,735] {processor.py:153} INFO - Started process (PID=98) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:27:55,736] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:27:55,736] {logging_mixin.py:115} INFO - [2022-06-23 10:27:55,736] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:27:55,740] {logging_mixin.py:115} INFO - [2022-06-23 10:27:55,740] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:27:55,741] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:27:55,755] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.022 seconds
[2022-06-23 10:28:25,836] {processor.py:153} INFO - Started process (PID=128) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:28:25,836] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:28:25,836] {logging_mixin.py:115} INFO - [2022-06-23 10:28:25,836] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:28:25,841] {logging_mixin.py:115} INFO - [2022-06-23 10:28:25,841] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:28:25,841] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:28:25,857] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.024 seconds
[2022-06-23 10:28:55,946] {processor.py:153} INFO - Started process (PID=158) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:28:55,946] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:28:55,946] {logging_mixin.py:115} INFO - [2022-06-23 10:28:55,946] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:28:55,952] {logging_mixin.py:115} INFO - [2022-06-23 10:28:55,951] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:28:55,952] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:28:55,965] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.022 seconds
[2022-06-23 10:29:26,046] {processor.py:153} INFO - Started process (PID=179) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:29:26,046] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:29:26,047] {logging_mixin.py:115} INFO - [2022-06-23 10:29:26,046] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:29:26,052] {logging_mixin.py:115} INFO - [2022-06-23 10:29:26,051] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:29:26,052] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:29:26,065] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.022 seconds
[2022-06-23 10:29:56,152] {processor.py:153} INFO - Started process (PID=210) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:29:56,152] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:29:56,153] {logging_mixin.py:115} INFO - [2022-06-23 10:29:56,153] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:29:56,158] {logging_mixin.py:115} INFO - [2022-06-23 10:29:56,158] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:29:56,158] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:29:56,171] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.023 seconds
[2022-06-23 10:30:26,252] {processor.py:153} INFO - Started process (PID=240) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:30:26,252] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:30:26,253] {logging_mixin.py:115} INFO - [2022-06-23 10:30:26,252] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:30:26,256] {logging_mixin.py:115} INFO - [2022-06-23 10:30:26,255] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:30:26,256] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:30:26,268] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.018 seconds
[2022-06-23 10:30:56,349] {processor.py:153} INFO - Started process (PID=271) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:30:56,350] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:30:56,350] {logging_mixin.py:115} INFO - [2022-06-23 10:30:56,350] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:30:56,354] {logging_mixin.py:115} INFO - [2022-06-23 10:30:56,353] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:30:56,354] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:30:56,369] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.021 seconds
[2022-06-23 10:31:26,448] {processor.py:153} INFO - Started process (PID=301) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:31:26,448] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:31:26,449] {logging_mixin.py:115} INFO - [2022-06-23 10:31:26,448] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:31:26,452] {logging_mixin.py:115} INFO - [2022-06-23 10:31:26,452] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:31:26,452] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:31:26,467] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.021 seconds
[2022-06-23 10:31:56,598] {processor.py:153} INFO - Started process (PID=322) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:31:56,598] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 10:31:56,599] {logging_mixin.py:115} INFO - [2022-06-23 10:31:56,599] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:31:56,602] {logging_mixin.py:115} INFO - [2022-06-23 10:31:56,602] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 10:31:56,603] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 10:31:56,616] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.021 seconds
[2022-06-23 13:47:36,975] {processor.py:153} INFO - Started process (PID=39) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:47:36,976] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:47:36,976] {logging_mixin.py:115} INFO - [2022-06-23 13:47:36,976] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:47:36,982] {logging_mixin.py:115} INFO - [2022-06-23 13:47:36,981] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 13:47:36,982] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:47:37,005] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.032 seconds
[2022-06-23 13:48:07,085] {processor.py:153} INFO - Started process (PID=70) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:48:07,085] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:48:07,085] {logging_mixin.py:115} INFO - [2022-06-23 13:48:07,085] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:48:07,089] {logging_mixin.py:115} INFO - [2022-06-23 13:48:07,089] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 13:48:07,089] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:48:07,105] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.022 seconds
[2022-06-23 13:48:37,188] {processor.py:153} INFO - Started process (PID=91) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:48:37,188] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:48:37,189] {logging_mixin.py:115} INFO - [2022-06-23 13:48:37,188] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:48:37,192] {logging_mixin.py:115} INFO - [2022-06-23 13:48:37,192] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 13:48:37,193] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:48:37,208] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.023 seconds
[2022-06-23 13:49:07,295] {processor.py:153} INFO - Started process (PID=121) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:49:07,295] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:49:07,296] {logging_mixin.py:115} INFO - [2022-06-23 13:49:07,296] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:49:07,303] {logging_mixin.py:115} INFO - [2022-06-23 13:49:07,302] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 13:49:07,303] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:49:07,319] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.029 seconds
[2022-06-23 13:49:37,400] {processor.py:153} INFO - Started process (PID=151) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:49:37,400] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:49:37,400] {logging_mixin.py:115} INFO - [2022-06-23 13:49:37,400] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:49:37,404] {logging_mixin.py:115} INFO - [2022-06-23 13:49:37,404] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 13:49:37,405] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:49:37,422] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.024 seconds
[2022-06-23 13:50:07,502] {processor.py:153} INFO - Started process (PID=184) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:07,502] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:50:07,502] {logging_mixin.py:115} INFO - [2022-06-23 13:50:07,502] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:07,508] {logging_mixin.py:115} INFO - [2022-06-23 13:50:07,507] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 9, in <module>
    from airflow.operators.python import TriggerDagRunOperator, TriggerRule
ImportError: cannot import name 'TriggerDagRunOperator' from 'airflow.operators.python' (/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py)
[2022-06-23 13:50:07,508] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:07,525] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.026 seconds
[2022-06-23 13:50:08,544] {processor.py:153} INFO - Started process (PID=186) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:08,545] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:50:08,546] {logging_mixin.py:115} INFO - [2022-06-23 13:50:08,546] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:08,739] {logging_mixin.py:115} INFO - [2022-06-23 13:50:08,738] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 2, in <module>
    from msilib import schema
ModuleNotFoundError: No module named 'msilib'
[2022-06-23 13:50:08,739] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:08,751] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.211 seconds
[2022-06-23 13:50:39,650] {processor.py:153} INFO - Started process (PID=216) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:39,650] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:50:39,651] {logging_mixin.py:115} INFO - [2022-06-23 13:50:39,651] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:39,761] {logging_mixin.py:115} INFO - [2022-06-23 13:50:39,760] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 2, in <module>
    from msilib import schema
ModuleNotFoundError: No module named 'msilib'
[2022-06-23 13:50:39,761] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:50:39,773] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.128 seconds
[2022-06-23 13:51:10,753] {processor.py:153} INFO - Started process (PID=246) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:51:10,757] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:51:10,757] {logging_mixin.py:115} INFO - [2022-06-23 13:51:10,757] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:51:10,870] {logging_mixin.py:115} INFO - [2022-06-23 13:51:10,870] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 2, in <module>
    from msilib import schema
ModuleNotFoundError: No module named 'msilib'
[2022-06-23 13:51:10,870] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:51:10,882] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.131 seconds
[2022-06-23 13:59:01,328] {processor.py:153} INFO - Started process (PID=38) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:59:01,329] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:59:01,329] {logging_mixin.py:115} INFO - [2022-06-23 13:59:01,329] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:59:01,757] {logging_mixin.py:115} INFO - [2022-06-23 13:59:01,755] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 5, in <module>
    from pyspark.sql import Functions as F
ImportError: cannot import name 'Functions' from 'pyspark.sql' (/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/__init__.py)
[2022-06-23 13:59:01,757] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:59:01,778] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.453 seconds
[2022-06-23 13:59:31,936] {processor.py:153} INFO - Started process (PID=69) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:59:31,936] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 13:59:31,937] {logging_mixin.py:115} INFO - [2022-06-23 13:59:31,937] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:59:32,125] {logging_mixin.py:115} INFO - [2022-06-23 13:59:32,125] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 5, in <module>
    from pyspark.sql import Functions as F
ImportError: cannot import name 'Functions' from 'pyspark.sql' (/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/__init__.py)
[2022-06-23 13:59:32,126] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 13:59:32,137] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.206 seconds
[2022-06-23 14:00:03,043] {processor.py:153} INFO - Started process (PID=102) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:00:03,043] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:00:03,043] {logging_mixin.py:115} INFO - [2022-06-23 14:00:03,043] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:00:06,622] {logging_mixin.py:115} INFO - [2022-06-23 14:00:06,621] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 47, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:00:06,623] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:00:06,643] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.602 seconds
[2022-06-23 14:01:52,311] {processor.py:153} INFO - Started process (PID=40) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:01:52,312] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:01:52,312] {logging_mixin.py:115} INFO - [2022-06-23 14:01:52,312] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:02:00,495] {logging_mixin.py:115} INFO - [2022-06-23 14:02:00,492] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 47, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:02:00,495] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:02:00,520] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.211 seconds
[2022-06-23 14:02:30,761] {processor.py:153} INFO - Started process (PID=228) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:02:30,761] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:02:30,762] {logging_mixin.py:115} INFO - [2022-06-23 14:02:30,761] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:02:33,776] {logging_mixin.py:115} INFO - [2022-06-23 14:02:33,775] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 47, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:02:33,776] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:02:33,794] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.035 seconds
[2022-06-23 14:03:03,866] {processor.py:153} INFO - Started process (PID=399) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:03:03,874] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:03:03,875] {logging_mixin.py:115} INFO - [2022-06-23 14:03:03,875] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:03:06,929] {logging_mixin.py:115} INFO - [2022-06-23 14:03:06,928] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 47, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:03:06,929] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:03:06,942] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.078 seconds
[2022-06-23 14:03:36,990] {processor.py:153} INFO - Started process (PID=568) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:03:36,991] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:03:36,991] {logging_mixin.py:115} INFO - [2022-06-23 14:03:36,991] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:03:40,042] {logging_mixin.py:115} INFO - [2022-06-23 14:03:40,041] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 47, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:03:40,042] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:03:40,055] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.071 seconds
[2022-06-23 14:04:10,100] {processor.py:153} INFO - Started process (PID=738) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:04:10,101] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:04:10,101] {logging_mixin.py:115} INFO - [2022-06-23 14:04:10,101] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:04:13,240] {logging_mixin.py:115} INFO - [2022-06-23 14:04:13,239] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 47, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:04:13,240] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:04:13,254] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.155 seconds
[2022-06-23 14:04:43,355] {processor.py:153} INFO - Started process (PID=909) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:04:43,357] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:04:43,358] {logging_mixin.py:115} INFO - [2022-06-23 14:04:43,358] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:04:46,720] {logging_mixin.py:115} INFO - [2022-06-23 14:04:46,719] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 47, in <module>
    "parquet_file": parquet_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:04:46,720] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:04:46,734] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.384 seconds
[2022-06-23 14:05:17,609] {processor.py:153} INFO - Started process (PID=1079) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:05:17,610] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:05:17,610] {logging_mixin.py:115} INFO - [2022-06-23 14:05:17,610] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:05:21,019] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:05:21,077] {logging_mixin.py:115} INFO - [2022-06-23 14:05:21,077] {manager.py:508} INFO - Created Permission View: can read on DAG:process-food-waste-data
[2022-06-23 14:05:21,084] {logging_mixin.py:115} INFO - [2022-06-23 14:05:21,083] {manager.py:508} INFO - Created Permission View: can delete on DAG:process-food-waste-data
[2022-06-23 14:05:21,089] {logging_mixin.py:115} INFO - [2022-06-23 14:05:21,089] {manager.py:508} INFO - Created Permission View: can edit on DAG:process-food-waste-data
[2022-06-23 14:05:21,090] {logging_mixin.py:115} INFO - [2022-06-23 14:05:21,089] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:05:21,094] {logging_mixin.py:115} INFO - [2022-06-23 14:05:21,094] {dag.py:2398} INFO - Creating ORM DAG for process-food-waste-data
[2022-06-23 14:05:21,101] {logging_mixin.py:115} INFO - [2022-06-23 14:05:21,101] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2020-01-01T00:00:00+00:00, run_after=2020-01-01T00:00:00+00:00
[2022-06-23 14:05:21,111] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.506 seconds
[2022-06-23 14:05:51,847] {processor.py:153} INFO - Started process (PID=1250) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:05:51,849] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:05:51,850] {logging_mixin.py:115} INFO - [2022-06-23 14:05:51,850] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:05:55,137] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:05:55,152] {logging_mixin.py:115} INFO - [2022-06-23 14:05:55,151] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:05:55,167] {logging_mixin.py:115} INFO - [2022-06-23 14:05:55,167] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2020-01-01T00:00:00+00:00, run_after=2020-01-01T00:00:00+00:00
[2022-06-23 14:05:55,177] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.336 seconds
[2022-06-23 14:06:47,691] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:06:47,692] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:06:47,692] {logging_mixin.py:115} INFO - [2022-06-23 14:06:47,692] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:06:55,182] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:06:55,209] {logging_mixin.py:115} INFO - [2022-06-23 14:06:55,208] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:06:55,239] {logging_mixin.py:115} INFO - [2022-06-23 14:06:55,239] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2020-01-01T00:00:00+00:00, run_after=2020-01-01T00:00:00+00:00
[2022-06-23 14:06:55,254] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.566 seconds
[2022-06-23 14:07:25,284] {processor.py:153} INFO - Started process (PID=225) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:07:25,284] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:07:25,284] {logging_mixin.py:115} INFO - [2022-06-23 14:07:25,284] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:07:28,316] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:07:28,329] {logging_mixin.py:115} INFO - [2022-06-23 14:07:28,329] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:07:28,341] {logging_mixin.py:115} INFO - [2022-06-23 14:07:28,341] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2020-01-01T00:00:00+00:00, run_after=2020-01-01T00:00:00+00:00
[2022-06-23 14:07:28,348] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.066 seconds
[2022-06-23 14:07:58,385] {processor.py:153} INFO - Started process (PID=394) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:07:58,386] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:07:58,386] {logging_mixin.py:115} INFO - [2022-06-23 14:07:58,386] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:08:01,559] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:08:01,572] {logging_mixin.py:115} INFO - [2022-06-23 14:08:01,572] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:08:01,583] {logging_mixin.py:115} INFO - [2022-06-23 14:08:01,583] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2020-01-01T00:00:00+00:00, run_after=2020-01-01T00:00:00+00:00
[2022-06-23 14:08:01,592] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.209 seconds
[2022-06-23 14:08:31,644] {processor.py:153} INFO - Started process (PID=568) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:08:31,644] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:08:31,645] {logging_mixin.py:115} INFO - [2022-06-23 14:08:31,645] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:08:36,117] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:08:36,138] {logging_mixin.py:115} INFO - [2022-06-23 14:08:36,137] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:08:36,167] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.526 seconds
[2022-06-23 14:09:06,906] {processor.py:153} INFO - Started process (PID=740) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:09:06,906] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:09:06,907] {logging_mixin.py:115} INFO - [2022-06-23 14:09:06,907] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:09:10,102] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:09:10,116] {logging_mixin.py:115} INFO - [2022-06-23 14:09:10,115] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:09:10,135] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.232 seconds
[2022-06-23 14:09:40,292] {processor.py:153} INFO - Started process (PID=910) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:09:40,294] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:09:40,295] {logging_mixin.py:115} INFO - [2022-06-23 14:09:40,295] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:09:43,606] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:09:43,621] {logging_mixin.py:115} INFO - [2022-06-23 14:09:43,620] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:09:43,646] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.359 seconds
[2022-06-23 14:18:56,794] {processor.py:153} INFO - Started process (PID=38) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:18:56,794] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:18:56,795] {logging_mixin.py:115} INFO - [2022-06-23 14:18:56,795] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:19:08,354] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:19:08,480] {logging_mixin.py:115} INFO - [2022-06-23 14:19:08,479] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:19:08,546] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 11.756 seconds
[2022-06-23 14:19:39,127] {processor.py:153} INFO - Started process (PID=235) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:19:39,128] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:19:39,128] {logging_mixin.py:115} INFO - [2022-06-23 14:19:39,128] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:19:43,202] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:19:43,219] {logging_mixin.py:115} INFO - [2022-06-23 14:19:43,219] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:19:43,244] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.123 seconds
[2022-06-23 14:20:38,005] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:20:38,005] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:20:38,006] {logging_mixin.py:115} INFO - [2022-06-23 14:20:38,005] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:20:45,864] {logging_mixin.py:115} INFO - [2022-06-23 14:20:45,862] {dagbag.py:412} ERROR - Failed to bag_dag: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 173, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 166, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 774, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 620, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 405, in _process_modules
    dag.timetable.validate()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 175, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2022-06-23 14:20:45,864] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:20:45,888] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.886 seconds
[2022-06-23 14:21:16,593] {processor.py:153} INFO - Started process (PID=222) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:21:16,594] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:21:16,594] {logging_mixin.py:115} INFO - [2022-06-23 14:21:16,594] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:21:19,594] {logging_mixin.py:115} INFO - [2022-06-23 14:21:19,594] {dagbag.py:412} ERROR - Failed to bag_dag: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 173, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 166, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 774, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 620, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 405, in _process_modules
    dag.timetable.validate()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 175, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2022-06-23 14:21:19,595] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:21:19,607] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.015 seconds
[2022-06-23 14:21:49,751] {processor.py:153} INFO - Started process (PID=393) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:21:49,761] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:21:49,761] {logging_mixin.py:115} INFO - [2022-06-23 14:21:49,761] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:21:52,878] {logging_mixin.py:115} INFO - [2022-06-23 14:21:52,878] {dagbag.py:412} ERROR - Failed to bag_dag: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 173, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 166, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 774, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 620, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 405, in _process_modules
    dag.timetable.validate()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 175, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2022-06-23 14:21:52,879] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:21:52,899] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.151 seconds
[2022-06-23 14:22:23,016] {processor.py:153} INFO - Started process (PID=564) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:22:23,016] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:22:23,016] {logging_mixin.py:115} INFO - [2022-06-23 14:22:23,016] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:22:26,147] {logging_mixin.py:115} INFO - [2022-06-23 14:22:26,146] {dagbag.py:412} ERROR - Failed to bag_dag: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 173, in validate
    croniter(self._expression)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 166, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 774, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/croniter/croniter.py", line 620, in _expand
    raise CroniterBadCronError(cls.bad_length)
croniter.croniter.CroniterBadCronError: Exactly 5 or 6 columns has to be specified for iterator expression.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 405, in _process_modules
    dag.timetable.validate()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/timetables/interval.py", line 175, in validate
    raise AirflowTimetableInvalid(str(e))
airflow.exceptions.AirflowTimetableInvalid: Exactly 5 or 6 columns has to be specified for iterator expression.
[2022-06-23 14:22:26,147] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:22:26,160] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.146 seconds
[2022-06-23 14:22:51,265] {processor.py:153} INFO - Started process (PID=735) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:22:51,265] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:22:51,266] {logging_mixin.py:115} INFO - [2022-06-23 14:22:51,266] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:22:54,758] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:22:54,791] {logging_mixin.py:115} INFO - [2022-06-23 14:22:54,791] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:22:54,814] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.551 seconds
[2022-06-23 14:23:58,458] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:23:58,459] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:23:58,460] {logging_mixin.py:115} INFO - [2022-06-23 14:23:58,459] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:24:07,039] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:24:07,072] {logging_mixin.py:115} INFO - [2022-06-23 14:24:07,072] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:24:07,123] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.667 seconds
[2022-06-23 14:24:37,212] {processor.py:153} INFO - Started process (PID=222) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:24:37,214] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:24:37,214] {logging_mixin.py:115} INFO - [2022-06-23 14:24:37,214] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:24:41,999] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:24:42,014] {logging_mixin.py:115} INFO - [2022-06-23 14:24:42,014] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:24:42,030] {logging_mixin.py:115} INFO - [2022-06-23 14:24:42,030] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-22T14:24:42.030111+00:00, run_after=2022-06-23T14:24:42.030111+00:00
[2022-06-23 14:24:42,039] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.829 seconds
[2022-06-23 14:25:12,168] {processor.py:153} INFO - Started process (PID=393) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:25:12,169] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:25:12,169] {logging_mixin.py:115} INFO - [2022-06-23 14:25:12,169] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:25:15,247] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:25:15,261] {logging_mixin.py:115} INFO - [2022-06-23 14:25:15,260] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:25:15,279] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.116 seconds
[2022-06-23 14:25:45,303] {processor.py:153} INFO - Started process (PID=562) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:25:45,305] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:25:45,305] {logging_mixin.py:115} INFO - [2022-06-23 14:25:45,305] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:25:48,454] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:25:48,467] {logging_mixin.py:115} INFO - [2022-06-23 14:25:48,466] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:25:48,486] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.186 seconds
[2022-06-23 14:26:18,537] {processor.py:153} INFO - Started process (PID=735) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:26:18,537] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:26:18,538] {logging_mixin.py:115} INFO - [2022-06-23 14:26:18,538] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:26:21,649] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:26:21,662] {logging_mixin.py:115} INFO - [2022-06-23 14:26:21,661] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:26:21,681] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.149 seconds
[2022-06-23 14:26:51,784] {processor.py:153} INFO - Started process (PID=905) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:26:51,785] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:26:51,785] {logging_mixin.py:115} INFO - [2022-06-23 14:26:51,785] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:26:54,916] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:26:54,929] {logging_mixin.py:115} INFO - [2022-06-23 14:26:54,929] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:26:54,947] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.166 seconds
[2022-06-23 14:27:25,062] {processor.py:153} INFO - Started process (PID=1076) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:27:25,063] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:27:25,063] {logging_mixin.py:115} INFO - [2022-06-23 14:27:25,063] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:27:28,218] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:27:28,231] {logging_mixin.py:115} INFO - [2022-06-23 14:27:28,231] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:27:28,256] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.195 seconds
[2022-06-23 14:27:58,363] {processor.py:153} INFO - Started process (PID=1246) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:27:58,363] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:27:58,364] {logging_mixin.py:115} INFO - [2022-06-23 14:27:58,364] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:28:01,572] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:28:01,586] {logging_mixin.py:115} INFO - [2022-06-23 14:28:01,586] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:28:01,615] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.256 seconds
[2022-06-23 14:28:31,711] {processor.py:153} INFO - Started process (PID=1420) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:28:31,711] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:28:31,712] {logging_mixin.py:115} INFO - [2022-06-23 14:28:31,712] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:28:35,136] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:28:35,149] {logging_mixin.py:115} INFO - [2022-06-23 14:28:35,149] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:28:35,171] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.463 seconds
[2022-06-23 14:29:05,225] {processor.py:153} INFO - Started process (PID=1596) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:29:05,227] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:29:05,227] {logging_mixin.py:115} INFO - [2022-06-23 14:29:05,227] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:29:08,487] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:29:08,500] {logging_mixin.py:115} INFO - [2022-06-23 14:29:08,500] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:29:08,528] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.307 seconds
[2022-06-23 14:29:39,329] {processor.py:153} INFO - Started process (PID=1778) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:29:39,330] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:29:39,330] {logging_mixin.py:115} INFO - [2022-06-23 14:29:39,330] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:29:42,808] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:29:42,823] {logging_mixin.py:115} INFO - [2022-06-23 14:29:42,823] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:29:42,847] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.520 seconds
[2022-06-23 14:30:13,604] {processor.py:153} INFO - Started process (PID=1947) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:30:13,604] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:30:13,605] {logging_mixin.py:115} INFO - [2022-06-23 14:30:13,605] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:30:16,748] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:30:16,761] {logging_mixin.py:115} INFO - [2022-06-23 14:30:16,760] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:30:16,772] {logging_mixin.py:115} INFO - [2022-06-23 14:30:16,772] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:30:16,789] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.188 seconds
[2022-06-23 14:30:46,848] {processor.py:153} INFO - Started process (PID=2118) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:30:46,848] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:30:46,848] {logging_mixin.py:115} INFO - [2022-06-23 14:30:46,848] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:30:49,845] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:30:49,859] {logging_mixin.py:115} INFO - [2022-06-23 14:30:49,859] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:30:49,870] {logging_mixin.py:115} INFO - [2022-06-23 14:30:49,870] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:30:49,888] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.042 seconds
[2022-06-23 14:31:19,951] {processor.py:153} INFO - Started process (PID=2287) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:31:19,953] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:31:19,953] {logging_mixin.py:115} INFO - [2022-06-23 14:31:19,953] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:31:22,996] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:31:23,009] {logging_mixin.py:115} INFO - [2022-06-23 14:31:23,009] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:31:23,021] {logging_mixin.py:115} INFO - [2022-06-23 14:31:23,021] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:31:23,038] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.088 seconds
[2022-06-23 14:31:53,068] {processor.py:153} INFO - Started process (PID=2457) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:31:53,068] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:31:53,068] {logging_mixin.py:115} INFO - [2022-06-23 14:31:53,068] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:31:56,226] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:31:56,240] {logging_mixin.py:115} INFO - [2022-06-23 14:31:56,239] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:31:56,252] {logging_mixin.py:115} INFO - [2022-06-23 14:31:56,252] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:31:56,261] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.197 seconds
[2022-06-23 14:32:26,332] {processor.py:153} INFO - Started process (PID=2627) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:32:26,332] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:32:26,332] {logging_mixin.py:115} INFO - [2022-06-23 14:32:26,332] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:32:29,512] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:32:29,525] {logging_mixin.py:115} INFO - [2022-06-23 14:32:29,524] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:32:29,536] {logging_mixin.py:115} INFO - [2022-06-23 14:32:29,536] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:32:29,544] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.215 seconds
[2022-06-23 14:32:59,710] {processor.py:153} INFO - Started process (PID=2796) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:32:59,710] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:32:59,710] {logging_mixin.py:115} INFO - [2022-06-23 14:32:59,710] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:33:02,874] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:33:02,887] {logging_mixin.py:115} INFO - [2022-06-23 14:33:02,887] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:33:02,898] {logging_mixin.py:115} INFO - [2022-06-23 14:33:02,898] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:33:02,911] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.203 seconds
[2022-06-23 14:33:32,999] {processor.py:153} INFO - Started process (PID=2968) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:33:32,999] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:33:33,000] {logging_mixin.py:115} INFO - [2022-06-23 14:33:33,000] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:33:36,146] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:33:36,159] {logging_mixin.py:115} INFO - [2022-06-23 14:33:36,159] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:33:36,170] {logging_mixin.py:115} INFO - [2022-06-23 14:33:36,170] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:33:36,177] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.179 seconds
[2022-06-23 14:34:06,286] {processor.py:153} INFO - Started process (PID=3140) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:34:06,287] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:34:06,287] {logging_mixin.py:115} INFO - [2022-06-23 14:34:06,287] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:34:09,384] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:34:09,400] {logging_mixin.py:115} INFO - [2022-06-23 14:34:09,399] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:34:09,412] {logging_mixin.py:115} INFO - [2022-06-23 14:34:09,412] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:34:09,420] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.137 seconds
[2022-06-23 14:34:39,529] {processor.py:153} INFO - Started process (PID=3310) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:34:39,529] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:34:39,530] {logging_mixin.py:115} INFO - [2022-06-23 14:34:39,530] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:34:43,639] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:34:43,653] {logging_mixin.py:115} INFO - [2022-06-23 14:34:43,652] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:34:43,665] {logging_mixin.py:115} INFO - [2022-06-23 14:34:43,665] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:34:43,674] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.147 seconds
[2022-06-23 14:35:13,804] {processor.py:153} INFO - Started process (PID=3484) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:35:13,805] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:35:13,805] {logging_mixin.py:115} INFO - [2022-06-23 14:35:13,805] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:35:16,976] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:35:16,989] {logging_mixin.py:115} INFO - [2022-06-23 14:35:16,989] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:35:17,000] {logging_mixin.py:115} INFO - [2022-06-23 14:35:17,000] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:35:17,007] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.205 seconds
[2022-06-23 14:35:47,065] {processor.py:153} INFO - Started process (PID=3655) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:35:47,066] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:35:47,066] {logging_mixin.py:115} INFO - [2022-06-23 14:35:47,066] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:35:50,315] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:35:50,329] {logging_mixin.py:115} INFO - [2022-06-23 14:35:50,329] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:35:50,342] {logging_mixin.py:115} INFO - [2022-06-23 14:35:50,341] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:35:50,349] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.286 seconds
[2022-06-23 14:36:21,178] {processor.py:153} INFO - Started process (PID=3836) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:36:21,179] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:36:21,179] {logging_mixin.py:115} INFO - [2022-06-23 14:36:21,179] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:36:24,192] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:36:24,205] {logging_mixin.py:115} INFO - [2022-06-23 14:36:24,205] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:36:24,216] {logging_mixin.py:115} INFO - [2022-06-23 14:36:24,216] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:36:24,226] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.051 seconds
[2022-06-23 14:36:54,284] {processor.py:153} INFO - Started process (PID=4005) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:36:54,293] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:36:54,294] {logging_mixin.py:115} INFO - [2022-06-23 14:36:54,294] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:36:57,431] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:36:57,444] {logging_mixin.py:115} INFO - [2022-06-23 14:36:57,444] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:36:57,456] {logging_mixin.py:115} INFO - [2022-06-23 14:36:57,456] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:36:57,473] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.193 seconds
[2022-06-23 14:37:27,568] {processor.py:153} INFO - Started process (PID=4178) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:37:27,569] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:37:27,569] {logging_mixin.py:115} INFO - [2022-06-23 14:37:27,569] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:37:30,555] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:37:30,568] {logging_mixin.py:115} INFO - [2022-06-23 14:37:30,568] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:37:30,580] {logging_mixin.py:115} INFO - [2022-06-23 14:37:30,580] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:37:30,596] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.030 seconds
[2022-06-23 14:38:00,672] {processor.py:153} INFO - Started process (PID=4348) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:38:00,673] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:38:00,673] {logging_mixin.py:115} INFO - [2022-06-23 14:38:00,673] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:38:03,687] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:38:03,700] {logging_mixin.py:115} INFO - [2022-06-23 14:38:03,700] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:38:03,712] {logging_mixin.py:115} INFO - [2022-06-23 14:38:03,712] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:38:03,719] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.050 seconds
[2022-06-23 14:38:33,787] {processor.py:153} INFO - Started process (PID=4518) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:38:33,788] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:38:33,789] {logging_mixin.py:115} INFO - [2022-06-23 14:38:33,788] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:38:36,899] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:38:36,912] {logging_mixin.py:115} INFO - [2022-06-23 14:38:36,912] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:38:36,923] {logging_mixin.py:115} INFO - [2022-06-23 14:38:36,923] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:38:36,932] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.149 seconds
[2022-06-23 14:39:07,043] {processor.py:153} INFO - Started process (PID=4691) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:39:07,043] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:39:07,043] {logging_mixin.py:115} INFO - [2022-06-23 14:39:07,043] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:39:10,072] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:39:10,086] {logging_mixin.py:115} INFO - [2022-06-23 14:39:10,085] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:39:10,097] {logging_mixin.py:115} INFO - [2022-06-23 14:39:10,097] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:39:10,103] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.062 seconds
[2022-06-23 14:39:40,190] {processor.py:153} INFO - Started process (PID=4860) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:39:40,190] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:39:40,190] {logging_mixin.py:115} INFO - [2022-06-23 14:39:40,190] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:39:43,296] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:39:43,311] {logging_mixin.py:115} INFO - [2022-06-23 14:39:43,311] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:39:43,323] {logging_mixin.py:115} INFO - [2022-06-23 14:39:43,322] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:39:43,340] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.154 seconds
[2022-06-23 14:40:13,443] {processor.py:153} INFO - Started process (PID=5031) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:40:13,444] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:40:13,444] {logging_mixin.py:115} INFO - [2022-06-23 14:40:13,444] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:40:16,591] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:40:16,604] {logging_mixin.py:115} INFO - [2022-06-23 14:40:16,604] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:40:16,615] {logging_mixin.py:115} INFO - [2022-06-23 14:40:16,615] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:40:16,622] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.181 seconds
[2022-06-23 14:40:46,720] {processor.py:153} INFO - Started process (PID=5201) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:40:46,720] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:40:46,721] {logging_mixin.py:115} INFO - [2022-06-23 14:40:46,721] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:40:49,934] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:40:49,947] {logging_mixin.py:115} INFO - [2022-06-23 14:40:49,947] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:40:49,959] {logging_mixin.py:115} INFO - [2022-06-23 14:40:49,959] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:40:49,975] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.256 seconds
[2022-06-23 14:41:20,033] {processor.py:153} INFO - Started process (PID=5374) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:41:20,042] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:41:20,043] {logging_mixin.py:115} INFO - [2022-06-23 14:41:20,042] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:41:23,244] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:41:23,260] {logging_mixin.py:115} INFO - [2022-06-23 14:41:23,260] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:41:23,273] {logging_mixin.py:115} INFO - [2022-06-23 14:41:23,273] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:41:23,281] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.249 seconds
[2022-06-23 14:41:53,441] {processor.py:153} INFO - Started process (PID=5546) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:41:53,442] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:41:53,442] {logging_mixin.py:115} INFO - [2022-06-23 14:41:53,442] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:41:56,617] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:41:56,630] {logging_mixin.py:115} INFO - [2022-06-23 14:41:56,630] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:41:56,641] {logging_mixin.py:115} INFO - [2022-06-23 14:41:56,641] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:41:56,648] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.212 seconds
[2022-06-23 14:42:26,698] {processor.py:153} INFO - Started process (PID=5716) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:42:26,698] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:42:26,698] {logging_mixin.py:115} INFO - [2022-06-23 14:42:26,698] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:42:29,847] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:42:29,859] {logging_mixin.py:115} INFO - [2022-06-23 14:42:29,859] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:42:29,870] {logging_mixin.py:115} INFO - [2022-06-23 14:42:29,870] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:42:29,879] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.183 seconds
[2022-06-23 14:42:59,961] {processor.py:153} INFO - Started process (PID=5886) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:42:59,961] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:42:59,962] {logging_mixin.py:115} INFO - [2022-06-23 14:42:59,962] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:43:03,152] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:43:03,166] {logging_mixin.py:115} INFO - [2022-06-23 14:43:03,166] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:43:03,178] {logging_mixin.py:115} INFO - [2022-06-23 14:43:03,178] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:43:03,185] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.227 seconds
[2022-06-23 14:43:33,213] {processor.py:153} INFO - Started process (PID=6064) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:43:33,214] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:43:33,215] {logging_mixin.py:115} INFO - [2022-06-23 14:43:33,215] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:43:36,463] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:43:36,478] {logging_mixin.py:115} INFO - [2022-06-23 14:43:36,478] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:43:36,491] {logging_mixin.py:115} INFO - [2022-06-23 14:43:36,491] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:43:36,502] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.291 seconds
[2022-06-23 14:44:07,337] {processor.py:153} INFO - Started process (PID=6235) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:44:07,339] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:44:07,339] {logging_mixin.py:115} INFO - [2022-06-23 14:44:07,339] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:44:10,554] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:44:10,567] {logging_mixin.py:115} INFO - [2022-06-23 14:44:10,567] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:44:10,578] {logging_mixin.py:115} INFO - [2022-06-23 14:44:10,578] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:44:10,586] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.251 seconds
[2022-06-23 14:44:40,685] {processor.py:153} INFO - Started process (PID=6406) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:44:40,686] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:44:40,686] {logging_mixin.py:115} INFO - [2022-06-23 14:44:40,686] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:44:43,978] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:44:43,992] {logging_mixin.py:115} INFO - [2022-06-23 14:44:43,991] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:44:44,003] {logging_mixin.py:115} INFO - [2022-06-23 14:44:44,003] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:44:44,019] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.337 seconds
[2022-06-23 14:45:14,119] {processor.py:153} INFO - Started process (PID=6578) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:45:14,120] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:45:14,120] {logging_mixin.py:115} INFO - [2022-06-23 14:45:14,120] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:45:17,351] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:45:17,364] {logging_mixin.py:115} INFO - [2022-06-23 14:45:17,363] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:45:17,376] {logging_mixin.py:115} INFO - [2022-06-23 14:45:17,376] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:45:17,385] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.270 seconds
[2022-06-23 14:45:47,497] {processor.py:153} INFO - Started process (PID=6750) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:45:47,497] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:45:47,497] {logging_mixin.py:115} INFO - [2022-06-23 14:45:47,497] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:45:50,647] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:45:50,660] {logging_mixin.py:115} INFO - [2022-06-23 14:45:50,660] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:45:50,671] {logging_mixin.py:115} INFO - [2022-06-23 14:45:50,671] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:45:50,687] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.192 seconds
[2022-06-23 14:46:20,781] {processor.py:153} INFO - Started process (PID=6919) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:46:20,782] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:46:20,782] {logging_mixin.py:115} INFO - [2022-06-23 14:46:20,782] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:46:23,916] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:46:23,928] {logging_mixin.py:115} INFO - [2022-06-23 14:46:23,928] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:46:23,940] {logging_mixin.py:115} INFO - [2022-06-23 14:46:23,940] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:46:23,951] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.172 seconds
[2022-06-23 14:46:54,102] {processor.py:153} INFO - Started process (PID=7091) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:46:54,102] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:46:54,103] {logging_mixin.py:115} INFO - [2022-06-23 14:46:54,103] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:46:57,098] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:46:57,111] {logging_mixin.py:115} INFO - [2022-06-23 14:46:57,111] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:46:57,122] {logging_mixin.py:115} INFO - [2022-06-23 14:46:57,122] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:46:57,132] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.033 seconds
[2022-06-23 14:47:27,195] {processor.py:153} INFO - Started process (PID=7261) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:47:27,196] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:47:27,197] {logging_mixin.py:115} INFO - [2022-06-23 14:47:27,197] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:47:30,245] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:47:30,258] {logging_mixin.py:115} INFO - [2022-06-23 14:47:30,257] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:47:30,269] {logging_mixin.py:115} INFO - [2022-06-23 14:47:30,269] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:47:30,278] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.087 seconds
[2022-06-23 14:48:00,304] {processor.py:153} INFO - Started process (PID=7431) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:48:00,304] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:48:00,304] {logging_mixin.py:115} INFO - [2022-06-23 14:48:00,304] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:48:03,364] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:48:03,377] {logging_mixin.py:115} INFO - [2022-06-23 14:48:03,377] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:48:03,392] {logging_mixin.py:115} INFO - [2022-06-23 14:48:03,392] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:48:03,401] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.101 seconds
[2022-06-23 14:48:33,606] {processor.py:153} INFO - Started process (PID=7601) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:48:33,607] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:48:33,608] {logging_mixin.py:115} INFO - [2022-06-23 14:48:33,608] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:48:37,040] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:48:37,052] {logging_mixin.py:115} INFO - [2022-06-23 14:48:37,052] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:48:37,063] {logging_mixin.py:115} INFO - [2022-06-23 14:48:37,063] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:48:37,071] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.467 seconds
[2022-06-23 14:49:07,749] {processor.py:153} INFO - Started process (PID=7771) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:49:07,750] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:49:07,751] {logging_mixin.py:115} INFO - [2022-06-23 14:49:07,751] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:49:10,997] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:49:11,010] {logging_mixin.py:115} INFO - [2022-06-23 14:49:11,009] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:49:11,021] {logging_mixin.py:115} INFO - [2022-06-23 14:49:11,021] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:49:11,034] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.289 seconds
[2022-06-23 14:49:41,121] {processor.py:153} INFO - Started process (PID=7951) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:49:41,121] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:49:41,121] {logging_mixin.py:115} INFO - [2022-06-23 14:49:41,121] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:49:44,356] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:49:44,369] {logging_mixin.py:115} INFO - [2022-06-23 14:49:44,368] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:49:44,380] {logging_mixin.py:115} INFO - [2022-06-23 14:49:44,380] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:49:44,388] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.269 seconds
[2022-06-23 14:50:14,490] {processor.py:153} INFO - Started process (PID=8122) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:50:14,491] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:50:14,492] {logging_mixin.py:115} INFO - [2022-06-23 14:50:14,492] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:50:17,621] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:50:17,635] {logging_mixin.py:115} INFO - [2022-06-23 14:50:17,634] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:50:17,646] {logging_mixin.py:115} INFO - [2022-06-23 14:50:17,646] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:50:17,654] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.167 seconds
[2022-06-23 14:50:47,750] {processor.py:153} INFO - Started process (PID=8292) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:50:47,751] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:50:47,751] {logging_mixin.py:115} INFO - [2022-06-23 14:50:47,751] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:50:50,881] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:50:50,896] {logging_mixin.py:115} INFO - [2022-06-23 14:50:50,896] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:50:50,907] {logging_mixin.py:115} INFO - [2022-06-23 14:50:50,907] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:50:50,914] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.166 seconds
[2022-06-23 14:51:21,010] {processor.py:153} INFO - Started process (PID=8465) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:51:21,011] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:51:21,012] {logging_mixin.py:115} INFO - [2022-06-23 14:51:21,012] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:51:24,000] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:51:24,014] {logging_mixin.py:115} INFO - [2022-06-23 14:51:24,013] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:51:24,025] {logging_mixin.py:115} INFO - [2022-06-23 14:51:24,025] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:51:24,032] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.027 seconds
[2022-06-23 14:51:54,121] {processor.py:153} INFO - Started process (PID=8635) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:51:54,121] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:51:54,121] {logging_mixin.py:115} INFO - [2022-06-23 14:51:54,121] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:51:57,718] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:51:57,733] {logging_mixin.py:115} INFO - [2022-06-23 14:51:57,733] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:51:57,747] {logging_mixin.py:115} INFO - [2022-06-23 14:51:57,746] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:51:57,755] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.637 seconds
[2022-06-23 14:52:27,852] {processor.py:153} INFO - Started process (PID=8804) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:52:27,852] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:52:27,852] {logging_mixin.py:115} INFO - [2022-06-23 14:52:27,852] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:52:31,535] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:52:31,549] {logging_mixin.py:115} INFO - [2022-06-23 14:52:31,548] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:52:31,561] {logging_mixin.py:115} INFO - [2022-06-23 14:52:31,560] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:52:31,569] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.720 seconds
[2022-06-23 14:53:02,189] {processor.py:153} INFO - Started process (PID=8978) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:53:02,190] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:53:02,190] {logging_mixin.py:115} INFO - [2022-06-23 14:53:02,190] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:53:05,702] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:53:05,716] {logging_mixin.py:115} INFO - [2022-06-23 14:53:05,716] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:53:05,729] {logging_mixin.py:115} INFO - [2022-06-23 14:53:05,729] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:53:05,736] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.550 seconds
[2022-06-23 14:53:36,522] {processor.py:153} INFO - Started process (PID=9150) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:53:36,522] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:53:36,523] {logging_mixin.py:115} INFO - [2022-06-23 14:53:36,523] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:53:39,993] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:53:40,009] {logging_mixin.py:115} INFO - [2022-06-23 14:53:40,009] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:53:40,023] {logging_mixin.py:115} INFO - [2022-06-23 14:53:40,023] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:53:40,037] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.520 seconds
[2022-06-23 14:54:10,786] {processor.py:153} INFO - Started process (PID=9319) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:54:10,786] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:54:10,786] {logging_mixin.py:115} INFO - [2022-06-23 14:54:10,786] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:54:13,933] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:54:13,947] {logging_mixin.py:115} INFO - [2022-06-23 14:54:13,946] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:54:13,957] {logging_mixin.py:115} INFO - [2022-06-23 14:54:13,957] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:54:13,967] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.183 seconds
[2022-06-23 14:54:44,053] {processor.py:153} INFO - Started process (PID=9491) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:54:44,055] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:54:44,055] {logging_mixin.py:115} INFO - [2022-06-23 14:54:44,055] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:54:47,239] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:54:47,253] {logging_mixin.py:115} INFO - [2022-06-23 14:54:47,252] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:54:47,264] {logging_mixin.py:115} INFO - [2022-06-23 14:54:47,263] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:54:47,272] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.220 seconds
[2022-06-23 14:55:17,356] {processor.py:153} INFO - Started process (PID=9671) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:55:17,356] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:55:17,356] {logging_mixin.py:115} INFO - [2022-06-23 14:55:17,356] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:55:20,768] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:55:20,783] {logging_mixin.py:115} INFO - [2022-06-23 14:55:20,783] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:55:20,796] {logging_mixin.py:115} INFO - [2022-06-23 14:55:20,796] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:55:20,805] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.451 seconds
[2022-06-23 14:55:50,903] {processor.py:153} INFO - Started process (PID=9843) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:55:50,905] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:55:50,906] {logging_mixin.py:115} INFO - [2022-06-23 14:55:50,905] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:55:53,942] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:55:53,956] {logging_mixin.py:115} INFO - [2022-06-23 14:55:53,955] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:55:53,967] {logging_mixin.py:115} INFO - [2022-06-23 14:55:53,967] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:55:53,976] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.079 seconds
[2022-06-23 14:56:24,068] {processor.py:153} INFO - Started process (PID=10012) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:56:24,069] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:56:24,070] {logging_mixin.py:115} INFO - [2022-06-23 14:56:24,069] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:56:27,391] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:56:27,404] {logging_mixin.py:115} INFO - [2022-06-23 14:56:27,404] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:56:27,417] {logging_mixin.py:115} INFO - [2022-06-23 14:56:27,417] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-23T14:29:27.819687+00:00, run_after=2022-06-24T14:29:27.819687+00:00
[2022-06-23 14:56:27,426] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.362 seconds
[2022-06-23 14:56:57,516] {processor.py:153} INFO - Started process (PID=10181) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:56:57,518] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:56:57,518] {logging_mixin.py:115} INFO - [2022-06-23 14:56:57,518] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:57:00,549] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:57:00,585] {logging_mixin.py:115} INFO - [2022-06-23 14:57:00,584] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 14:57:00,589] {logging_mixin.py:115} INFO - [2022-06-23 14:57:00,589] {dag.py:2398} INFO - Creating ORM DAG for process-food-waste-data
[2022-06-23 14:57:00,595] {logging_mixin.py:115} INFO - [2022-06-23 14:57:00,595] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to 2022-06-22T14:57:00.595496+00:00, run_after=2022-06-23T14:57:00.595496+00:00
[2022-06-23 14:57:00,605] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.090 seconds
[2022-06-23 14:57:28,697] {processor.py:153} INFO - Started process (PID=10351) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:57:28,698] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:57:28,698] {logging_mixin.py:115} INFO - [2022-06-23 14:57:28,698] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:57:31,938] {logging_mixin.py:115} INFO - [2022-06-23 14:57:31,937] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py", line 147, in __init__
    conf, jsc, profiler_cls)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py", line 219, in _do_init
    self._jsc.sc().register(self._javaAccumulator)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1320, in __call__
    answer = self.gateway_client.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/manager.py", line 463, in _exit_gracefully
    self.log.debug("Current Stacktrace is: %s", '\n'.join(map(str, inspect.stack())))
  File "/usr/local/lib/python3.7/inspect.py", line 1513, in stack
    return getouterframes(sys._getframe(1), context)
  File "/usr/local/lib/python3.7/inspect.py", line 1490, in getouterframes
    frameinfo = (frame,) + getframeinfo(frame, context)
  File "/usr/local/lib/python3.7/inspect.py", line 1464, in getframeinfo
    lines, lnum = findsource(frame)
  File "/usr/local/lib/python3.7/inspect.py", line 780, in findsource
    module = getmodule(object, file)
  File "/usr/local/lib/python3.7/inspect.py", line 742, in getmodule
    os.path.realpath(f)] = module.__name__
  File "/usr/local/lib/python3.7/posixpath.py", line 395, in realpath
    path, ok = _joinrealpath(filename[:0], filename, {})
  File "/usr/local/lib/python3.7/posixpath.py", line 429, in _joinrealpath
    if not islink(newpath):
  File "/usr/local/lib/python3.7/posixpath.py", line 171, in islink
    st = os.lstat(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/manager.py", line 467, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
[2022-06-23 14:57:31,938] {logging_mixin.py:115} INFO - [2022-06-23 14:57:31,938] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 14:57:31,938] {logging_mixin.py:115} INFO - [2022-06-23 14:57:31,938] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py", line 147, in __init__
    conf, jsc, profiler_cls)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py", line 219, in _do_init
    self._jsc.sc().register(self._javaAccumulator)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1320, in __call__
    answer = self.gateway_client.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/manager.py", line 463, in _exit_gracefully
    self.log.debug("Current Stacktrace is: %s", '\n'.join(map(str, inspect.stack())))
  File "/usr/local/lib/python3.7/inspect.py", line 1513, in stack
    return getouterframes(sys._getframe(1), context)
  File "/usr/local/lib/python3.7/inspect.py", line 1490, in getouterframes
    frameinfo = (frame,) + getframeinfo(frame, context)
  File "/usr/local/lib/python3.7/inspect.py", line 1464, in getframeinfo
    lines, lnum = findsource(frame)
  File "/usr/local/lib/python3.7/inspect.py", line 780, in findsource
    module = getmodule(object, file)
  File "/usr/local/lib/python3.7/inspect.py", line 742, in getmodule
    os.path.realpath(f)] = module.__name__
  File "/usr/local/lib/python3.7/posixpath.py", line 395, in realpath
    path, ok = _joinrealpath(filename[:0], filename, {})
  File "/usr/local/lib/python3.7/posixpath.py", line 429, in _joinrealpath
    if not islink(newpath):
  File "/usr/local/lib/python3.7/posixpath.py", line 171, in islink
    st = os.lstat(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/manager.py", line 467, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 14:57:31,942] {logging_mixin.py:115} WARNING - /opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/context.py:472 RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.
[2022-06-23 14:58:05,638] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:58:05,639] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:58:05,639] {logging_mixin.py:115} INFO - [2022-06-23 14:58:05,639] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:58:15,256] {logging_mixin.py:115} INFO - [2022-06-23 14:58:15,254] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:58:15,256] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:58:15,276] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 9.641 seconds
[2022-06-23 14:58:45,358] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:58:45,359] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:58:45,360] {logging_mixin.py:115} INFO - [2022-06-23 14:58:45,360] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:58:48,688] {logging_mixin.py:115} INFO - [2022-06-23 14:58:48,687] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:58:48,688] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:58:48,702] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.345 seconds
[2022-06-23 14:59:18,741] {processor.py:153} INFO - Started process (PID=393) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:59:18,743] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:59:18,744] {logging_mixin.py:115} INFO - [2022-06-23 14:59:18,744] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:59:21,926] {logging_mixin.py:115} INFO - [2022-06-23 14:59:21,925] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:59:21,926] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:59:21,941] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.204 seconds
[2022-06-23 14:59:51,980] {processor.py:153} INFO - Started process (PID=562) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:59:51,980] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 14:59:51,981] {logging_mixin.py:115} INFO - [2022-06-23 14:59:51,981] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:59:55,147] {logging_mixin.py:115} INFO - [2022-06-23 14:59:55,146] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 14:59:55,147] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 14:59:55,160] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.184 seconds
[2022-06-23 15:00:25,272] {processor.py:153} INFO - Started process (PID=734) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:00:25,273] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:00:25,274] {logging_mixin.py:115} INFO - [2022-06-23 15:00:25,274] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:00:28,590] {logging_mixin.py:115} INFO - [2022-06-23 15:00:28,588] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 15:00:28,590] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:00:28,604] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.336 seconds
[2022-06-23 15:00:59,381] {processor.py:153} INFO - Started process (PID=903) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:00:59,390] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:00:59,391] {logging_mixin.py:115} INFO - [2022-06-23 15:00:59,391] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:01:02,664] {logging_mixin.py:115} INFO - [2022-06-23 15:01:02,662] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 15:01:02,665] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:01:02,678] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.301 seconds
[2022-06-23 15:01:32,777] {processor.py:153} INFO - Started process (PID=1076) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:01:32,779] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:01:32,780] {logging_mixin.py:115} INFO - [2022-06-23 15:01:32,780] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:01:36,390] {logging_mixin.py:115} INFO - [2022-06-23 15:01:36,388] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 15:01:36,390] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:01:36,404] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.632 seconds
[2022-06-23 15:02:06,898] {processor.py:153} INFO - Started process (PID=1261) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:02:06,900] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:02:06,901] {logging_mixin.py:115} INFO - [2022-06-23 15:02:06,900] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:02:10,235] {logging_mixin.py:115} INFO - [2022-06-23 15:02:10,234] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 45, in <module>
    "cols_to_drop": ["image_id", "id"]
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 156, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 880, in __init__
    self.dag = dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 990, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 1048, in dag
    dag.add_task(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py", line 2130, in add_task
    raise AirflowException("DAG is missing the start_date parameter")
airflow.exceptions.AirflowException: DAG is missing the start_date parameter
[2022-06-23 15:02:10,235] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:02:10,249] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.357 seconds
[2022-06-23 15:02:14,114] {processor.py:153} INFO - Started process (PID=1403) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:02:14,114] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:02:14,115] {logging_mixin.py:115} INFO - [2022-06-23 15:02:14,115] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:02:58,777] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:02:58,777] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:02:58,778] {logging_mixin.py:115} INFO - [2022-06-23 15:02:58,778] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:03:07,370] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:03:07,459] {logging_mixin.py:115} INFO - [2022-06-23 15:03:07,458] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:03:07,488] {logging_mixin.py:115} INFO - [2022-06-23 15:03:07,488] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:03:07,512] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.738 seconds
[2022-06-23 15:03:37,648] {processor.py:153} INFO - Started process (PID=223) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:03:37,648] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:03:37,649] {logging_mixin.py:115} INFO - [2022-06-23 15:03:37,649] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:03:40,905] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:03:40,919] {logging_mixin.py:115} INFO - [2022-06-23 15:03:40,918] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:03:40,930] {logging_mixin.py:115} INFO - [2022-06-23 15:03:40,930] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:03:40,940] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.295 seconds
[2022-06-23 15:04:11,757] {processor.py:153} INFO - Started process (PID=394) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:04:11,759] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:04:11,760] {logging_mixin.py:115} INFO - [2022-06-23 15:04:11,760] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:04:14,878] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:04:14,891] {logging_mixin.py:115} INFO - [2022-06-23 15:04:14,890] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:04:14,902] {logging_mixin.py:115} INFO - [2022-06-23 15:04:14,902] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:04:14,911] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.156 seconds
[2022-06-23 15:04:45,008] {processor.py:153} INFO - Started process (PID=571) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:04:45,009] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:04:45,009] {logging_mixin.py:115} INFO - [2022-06-23 15:04:45,009] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:04:49,079] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:04:49,096] {logging_mixin.py:115} INFO - [2022-06-23 15:04:49,095] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:04:49,121] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.118 seconds
[2022-06-23 15:05:19,304] {processor.py:153} INFO - Started process (PID=740) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:05:19,305] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:05:19,305] {logging_mixin.py:115} INFO - [2022-06-23 15:05:19,305] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:05:22,453] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:05:22,466] {logging_mixin.py:115} INFO - [2022-06-23 15:05:22,466] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:05:22,488] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.185 seconds
[2022-06-23 15:05:33,568] {processor.py:153} INFO - Started process (PID=893) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:05:33,569] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:05:33,569] {logging_mixin.py:115} INFO - [2022-06-23 15:05:33,569] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:05:37,276] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:05:37,313] {logging_mixin.py:115} INFO - [2022-06-23 15:05:37,313] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:05:37,338] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.772 seconds
[2022-06-23 15:06:07,432] {processor.py:153} INFO - Started process (PID=1075) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:06:07,433] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:06:07,434] {logging_mixin.py:115} INFO - [2022-06-23 15:06:07,434] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:06:10,533] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:06:10,547] {logging_mixin.py:115} INFO - [2022-06-23 15:06:10,547] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:06:10,569] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.140 seconds
[2022-06-23 15:06:40,663] {processor.py:153} INFO - Started process (PID=1245) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:06:40,664] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:06:40,664] {logging_mixin.py:115} INFO - [2022-06-23 15:06:40,664] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:06:43,956] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:06:43,970] {logging_mixin.py:115} INFO - [2022-06-23 15:06:43,970] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:06:43,993] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.335 seconds
[2022-06-23 15:07:14,086] {processor.py:153} INFO - Started process (PID=1416) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:07:14,086] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:07:14,086] {logging_mixin.py:115} INFO - [2022-06-23 15:07:14,086] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:07:17,589] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:07:17,604] {logging_mixin.py:115} INFO - [2022-06-23 15:07:17,603] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:07:17,625] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.541 seconds
[2022-06-23 15:07:47,720] {processor.py:153} INFO - Started process (PID=1587) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:07:47,720] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:07:47,720] {logging_mixin.py:115} INFO - [2022-06-23 15:07:47,720] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:07:50,791] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:07:50,804] {logging_mixin.py:115} INFO - [2022-06-23 15:07:50,804] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:07:50,822] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.106 seconds
[2022-06-23 15:08:20,912] {processor.py:153} INFO - Started process (PID=1756) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:08:20,912] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:08:20,913] {logging_mixin.py:115} INFO - [2022-06-23 15:08:20,913] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:08:24,054] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:08:24,068] {logging_mixin.py:115} INFO - [2022-06-23 15:08:24,067] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:08:24,086] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.175 seconds
[2022-06-23 15:08:54,188] {processor.py:153} INFO - Started process (PID=1928) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:08:54,189] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:08:54,189] {logging_mixin.py:115} INFO - [2022-06-23 15:08:54,189] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:08:57,310] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:08:57,323] {logging_mixin.py:115} INFO - [2022-06-23 15:08:57,322] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:08:57,341] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.156 seconds
[2022-06-23 15:09:27,441] {processor.py:153} INFO - Started process (PID=2100) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:09:27,441] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:09:27,442] {logging_mixin.py:115} INFO - [2022-06-23 15:09:27,442] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:09:30,847] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:09:30,860] {logging_mixin.py:115} INFO - [2022-06-23 15:09:30,860] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:09:30,879] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.444 seconds
[2022-06-23 15:10:00,982] {processor.py:153} INFO - Started process (PID=2270) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:10:00,982] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:10:00,983] {logging_mixin.py:115} INFO - [2022-06-23 15:10:00,983] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:10:04,133] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:10:04,148] {logging_mixin.py:115} INFO - [2022-06-23 15:10:04,147] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:10:04,163] {logging_mixin.py:115} INFO - [2022-06-23 15:10:04,163] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:10:04,174] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.196 seconds
[2022-06-23 15:10:34,263] {processor.py:153} INFO - Started process (PID=2440) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:10:34,264] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:10:34,264] {logging_mixin.py:115} INFO - [2022-06-23 15:10:34,264] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:10:37,512] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:10:37,526] {logging_mixin.py:115} INFO - [2022-06-23 15:10:37,525] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:10:37,539] {logging_mixin.py:115} INFO - [2022-06-23 15:10:37,539] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:10:37,548] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.287 seconds
[2022-06-23 15:12:47,521] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:12:47,522] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:12:47,522] {logging_mixin.py:115} INFO - [2022-06-23 15:12:47,522] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:12:55,715] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:12:55,735] {logging_mixin.py:115} INFO - [2022-06-23 15:12:55,735] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:12:55,756] {logging_mixin.py:115} INFO - [2022-06-23 15:12:55,756] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:12:55,769] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.251 seconds
[2022-06-23 15:13:26,169] {processor.py:153} INFO - Started process (PID=223) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:13:26,170] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:13:26,170] {logging_mixin.py:115} INFO - [2022-06-23 15:13:26,170] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:13:29,428] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:13:29,441] {logging_mixin.py:115} INFO - [2022-06-23 15:13:29,440] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:13:29,452] {logging_mixin.py:115} INFO - [2022-06-23 15:13:29,452] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:13:29,460] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.295 seconds
[2022-06-23 15:13:59,543] {processor.py:153} INFO - Started process (PID=400) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:13:59,545] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:13:59,545] {logging_mixin.py:115} INFO - [2022-06-23 15:13:59,545] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:14:04,216] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:14:04,232] {logging_mixin.py:115} INFO - [2022-06-23 15:14:04,232] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:14:04,257] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.716 seconds
[2022-06-23 15:14:34,657] {processor.py:153} INFO - Started process (PID=573) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:14:34,658] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:14:34,659] {logging_mixin.py:115} INFO - [2022-06-23 15:14:34,659] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:14:37,803] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:14:37,815] {logging_mixin.py:115} INFO - [2022-06-23 15:14:37,815] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:14:37,834] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.181 seconds
[2022-06-23 15:15:07,892] {processor.py:153} INFO - Started process (PID=744) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:15:07,894] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:15:07,895] {logging_mixin.py:115} INFO - [2022-06-23 15:15:07,895] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:15:11,107] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:15:11,123] {logging_mixin.py:115} INFO - [2022-06-23 15:15:11,123] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:15:11,144] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.254 seconds
[2022-06-23 15:15:41,253] {processor.py:153} INFO - Started process (PID=914) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:15:41,253] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:15:41,254] {logging_mixin.py:115} INFO - [2022-06-23 15:15:41,254] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:15:44,727] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:15:44,740] {logging_mixin.py:115} INFO - [2022-06-23 15:15:44,740] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:15:44,758] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.506 seconds
[2022-06-23 15:16:15,370] {processor.py:153} INFO - Started process (PID=1086) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:16:15,370] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:16:15,370] {logging_mixin.py:115} INFO - [2022-06-23 15:16:15,370] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:16:18,658] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:16:18,672] {logging_mixin.py:115} INFO - [2022-06-23 15:16:18,671] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:16:18,692] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.327 seconds
[2022-06-23 15:16:48,729] {processor.py:153} INFO - Started process (PID=1256) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:16:48,730] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:16:48,731] {logging_mixin.py:115} INFO - [2022-06-23 15:16:48,731] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:16:51,981] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:16:51,995] {logging_mixin.py:115} INFO - [2022-06-23 15:16:51,995] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:16:52,014] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.287 seconds
[2022-06-23 15:17:22,094] {processor.py:153} INFO - Started process (PID=1432) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:17:22,095] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:17:22,095] {logging_mixin.py:115} INFO - [2022-06-23 15:17:22,095] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:17:25,431] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:17:25,445] {logging_mixin.py:115} INFO - [2022-06-23 15:17:25,444] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:17:25,465] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.372 seconds
[2022-06-23 15:17:56,205] {processor.py:153} INFO - Started process (PID=1613) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:17:56,207] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:17:56,207] {logging_mixin.py:115} INFO - [2022-06-23 15:17:56,207] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:17:59,296] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:17:59,309] {logging_mixin.py:115} INFO - [2022-06-23 15:17:59,308] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:17:59,327] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.124 seconds
[2022-06-23 15:18:29,477] {processor.py:153} INFO - Started process (PID=1784) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:18:29,478] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:18:29,478] {logging_mixin.py:115} INFO - [2022-06-23 15:18:29,478] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:18:32,686] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:18:32,698] {logging_mixin.py:115} INFO - [2022-06-23 15:18:32,698] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:18:32,721] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.249 seconds
[2022-06-23 15:19:02,741] {processor.py:153} INFO - Started process (PID=1954) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:19:02,742] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:19:02,742] {logging_mixin.py:115} INFO - [2022-06-23 15:19:02,742] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:19:06,885] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:19:06,904] {logging_mixin.py:115} INFO - [2022-06-23 15:19:06,904] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:19:06,958] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.218 seconds
[2022-06-23 15:19:37,038] {processor.py:153} INFO - Started process (PID=2126) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:19:37,038] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:19:37,038] {logging_mixin.py:115} INFO - [2022-06-23 15:19:37,038] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:19:41,013] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:19:41,032] {logging_mixin.py:115} INFO - [2022-06-23 15:19:41,031] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:19:41,049] {logging_mixin.py:115} INFO - [2022-06-23 15:19:41,049] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:19:41,061] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.026 seconds
[2022-06-23 15:20:11,145] {processor.py:153} INFO - Started process (PID=2298) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:20:11,146] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:20:11,147] {logging_mixin.py:115} INFO - [2022-06-23 15:20:11,147] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:20:14,830] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:20:14,847] {logging_mixin.py:115} INFO - [2022-06-23 15:20:14,846] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:20:14,864] {logging_mixin.py:115} INFO - [2022-06-23 15:20:14,864] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:20:14,875] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.733 seconds
[2022-06-23 15:20:45,426] {processor.py:153} INFO - Started process (PID=2469) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:20:45,428] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:20:45,428] {logging_mixin.py:115} INFO - [2022-06-23 15:20:45,428] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:20:48,661] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:20:48,674] {logging_mixin.py:115} INFO - [2022-06-23 15:20:48,673] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:20:48,685] {logging_mixin.py:115} INFO - [2022-06-23 15:20:48,685] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:20:48,693] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.269 seconds
[2022-06-23 15:21:18,714] {processor.py:153} INFO - Started process (PID=2638) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:21:18,714] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:21:18,714] {logging_mixin.py:115} INFO - [2022-06-23 15:21:18,714] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:21:22,000] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:21:22,012] {logging_mixin.py:115} INFO - [2022-06-23 15:21:22,011] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:21:22,023] {logging_mixin.py:115} INFO - [2022-06-23 15:21:22,023] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:21:22,030] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.318 seconds
[2022-06-23 15:21:52,122] {processor.py:153} INFO - Started process (PID=2810) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:21:52,122] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:21:52,123] {logging_mixin.py:115} INFO - [2022-06-23 15:21:52,123] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:21:55,730] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:21:55,747] {logging_mixin.py:115} INFO - [2022-06-23 15:21:55,747] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:21:55,762] {logging_mixin.py:115} INFO - [2022-06-23 15:21:55,762] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:21:55,773] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.655 seconds
[2022-06-23 15:22:26,487] {processor.py:153} INFO - Started process (PID=2983) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:22:26,487] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:22:26,487] {logging_mixin.py:115} INFO - [2022-06-23 15:22:26,487] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:22:30,705] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:22:30,721] {logging_mixin.py:115} INFO - [2022-06-23 15:22:30,720] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:22:30,738] {logging_mixin.py:115} INFO - [2022-06-23 15:22:30,738] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:22:30,749] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.264 seconds
[2022-06-23 15:23:00,804] {processor.py:153} INFO - Started process (PID=3157) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:23:00,804] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:23:00,805] {logging_mixin.py:115} INFO - [2022-06-23 15:23:00,805] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:23:07,231] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:23:07,252] {logging_mixin.py:115} INFO - [2022-06-23 15:23:07,252] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:23:07,273] {logging_mixin.py:115} INFO - [2022-06-23 15:23:07,272] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:23:07,292] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 6.490 seconds
[2022-06-23 15:26:59,686] {processor.py:153} INFO - Started process (PID=38) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:26:59,687] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:26:59,687] {logging_mixin.py:115} INFO - [2022-06-23 15:26:59,687] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:27:07,856] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:27:07,877] {logging_mixin.py:115} INFO - [2022-06-23 15:27:07,877] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:27:07,902] {logging_mixin.py:115} INFO - [2022-06-23 15:27:07,902] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:27:07,926] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.244 seconds
[2022-06-23 15:27:38,271] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:27:38,271] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:27:38,271] {logging_mixin.py:115} INFO - [2022-06-23 15:27:38,271] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:27:41,498] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:27:41,512] {logging_mixin.py:115} INFO - [2022-06-23 15:27:41,511] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 15:27:41,524] {logging_mixin.py:115} INFO - [2022-06-23 15:27:41,524] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 15:27:41,532] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.263 seconds
[2022-06-23 15:30:26,838] {processor.py:153} INFO - Started process (PID=38) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:30:26,839] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:30:26,839] {logging_mixin.py:115} INFO - [2022-06-23 15:30:26,839] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:30:33,244] {logging_mixin.py:115} INFO - [2022-06-23 15:30:33,239] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:30:33,244] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:30:33,268] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 6.434 seconds
[2022-06-23 15:31:03,456] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:31:03,456] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:31:03,457] {logging_mixin.py:115} INFO - [2022-06-23 15:31:03,456] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:31:06,253] {logging_mixin.py:115} INFO - [2022-06-23 15:31:06,251] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:31:06,253] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:31:06,266] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.811 seconds
[2022-06-23 15:31:36,621] {processor.py:153} INFO - Started process (PID=398) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:31:36,621] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:31:36,621] {logging_mixin.py:115} INFO - [2022-06-23 15:31:36,621] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:31:39,705] {logging_mixin.py:115} INFO - [2022-06-23 15:31:39,703] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:31:39,705] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:31:39,720] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.100 seconds
[2022-06-23 15:32:09,887] {processor.py:153} INFO - Started process (PID=571) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:32:09,888] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:32:09,888] {logging_mixin.py:115} INFO - [2022-06-23 15:32:09,888] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:32:12,527] {logging_mixin.py:115} INFO - [2022-06-23 15:32:12,526] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:32:12,528] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:32:12,541] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.659 seconds
[2022-06-23 15:32:42,986] {processor.py:153} INFO - Started process (PID=739) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:32:42,987] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:32:42,988] {logging_mixin.py:115} INFO - [2022-06-23 15:32:42,988] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:32:45,595] {logging_mixin.py:115} INFO - [2022-06-23 15:32:45,593] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:32:45,595] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:32:45,607] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.623 seconds
[2022-06-23 15:33:16,257] {processor.py:153} INFO - Started process (PID=910) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:33:16,258] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:33:16,258] {logging_mixin.py:115} INFO - [2022-06-23 15:33:16,258] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:33:18,749] {logging_mixin.py:115} INFO - [2022-06-23 15:33:18,748] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:33:18,750] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:33:18,761] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.506 seconds
[2022-06-23 15:33:49,515] {processor.py:153} INFO - Started process (PID=1078) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:33:49,518] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:33:49,518] {logging_mixin.py:115} INFO - [2022-06-23 15:33:49,518] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:33:52,059] {logging_mixin.py:115} INFO - [2022-06-23 15:33:52,057] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:33:52,059] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:33:52,088] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.578 seconds
[2022-06-23 15:34:22,778] {processor.py:153} INFO - Started process (PID=1246) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:34:22,778] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:34:22,778] {logging_mixin.py:115} INFO - [2022-06-23 15:34:22,778] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:34:25,392] {logging_mixin.py:115} INFO - [2022-06-23 15:34:25,391] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:34:25,393] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:34:25,406] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.630 seconds
[2022-06-23 15:34:55,990] {processor.py:153} INFO - Started process (PID=1416) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:34:55,990] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:34:55,991] {logging_mixin.py:115} INFO - [2022-06-23 15:34:55,991] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:34:58,504] {logging_mixin.py:115} INFO - [2022-06-23 15:34:58,502] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:34:58,504] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:34:58,517] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.531 seconds
[2022-06-23 15:35:29,263] {processor.py:153} INFO - Started process (PID=1585) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:35:29,263] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:35:29,264] {logging_mixin.py:115} INFO - [2022-06-23 15:35:29,263] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:35:31,781] {logging_mixin.py:115} INFO - [2022-06-23 15:35:31,779] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:35:31,781] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:35:31,795] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.534 seconds
[2022-06-23 15:36:02,567] {processor.py:153} INFO - Started process (PID=1755) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:36:02,568] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:36:02,568] {logging_mixin.py:115} INFO - [2022-06-23 15:36:02,568] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:36:05,057] {logging_mixin.py:115} INFO - [2022-06-23 15:36:05,055] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:36:05,057] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:36:05,072] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.506 seconds
[2022-06-23 15:36:35,846] {processor.py:153} INFO - Started process (PID=1925) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:36:35,847] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:36:35,847] {logging_mixin.py:115} INFO - [2022-06-23 15:36:35,847] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:36:38,419] {logging_mixin.py:115} INFO - [2022-06-23 15:36:38,416] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:36:38,419] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:36:38,434] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.591 seconds
[2022-06-23 15:37:09,089] {processor.py:153} INFO - Started process (PID=2096) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:37:09,089] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:37:09,089] {logging_mixin.py:115} INFO - [2022-06-23 15:37:09,089] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:37:11,694] {logging_mixin.py:115} INFO - [2022-06-23 15:37:11,692] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:37:11,694] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:37:11,707] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.620 seconds
[2022-06-23 15:37:42,346] {processor.py:153} INFO - Started process (PID=2265) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:37:42,346] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:37:42,347] {logging_mixin.py:115} INFO - [2022-06-23 15:37:42,347] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:37:45,073] {logging_mixin.py:115} INFO - [2022-06-23 15:37:45,070] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:37:45,073] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:37:45,086] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.743 seconds
[2022-06-23 15:38:15,602] {processor.py:153} INFO - Started process (PID=2442) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:38:15,602] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:38:15,602] {logging_mixin.py:115} INFO - [2022-06-23 15:38:15,602] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:38:18,248] {logging_mixin.py:115} INFO - [2022-06-23 15:38:18,247] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:38:18,249] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:38:18,261] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.661 seconds
[2022-06-23 15:38:48,834] {processor.py:153} INFO - Started process (PID=2613) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:38:48,834] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:38:48,835] {logging_mixin.py:115} INFO - [2022-06-23 15:38:48,835] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:38:51,354] {logging_mixin.py:115} INFO - [2022-06-23 15:38:51,352] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:38:51,354] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:38:51,366] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.534 seconds
[2022-06-23 15:39:22,118] {processor.py:153} INFO - Started process (PID=2781) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:39:22,127] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:39:22,128] {logging_mixin.py:115} INFO - [2022-06-23 15:39:22,128] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:39:24,740] {logging_mixin.py:115} INFO - [2022-06-23 15:39:24,738] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:39:24,740] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:39:24,753] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.639 seconds
[2022-06-23 15:39:55,332] {processor.py:153} INFO - Started process (PID=2950) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:39:55,332] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:39:55,333] {logging_mixin.py:115} INFO - [2022-06-23 15:39:55,333] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:39:57,913] {logging_mixin.py:115} INFO - [2022-06-23 15:39:57,912] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:39:57,914] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:39:57,927] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.597 seconds
[2022-06-23 15:40:28,611] {processor.py:153} INFO - Started process (PID=3119) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:40:28,611] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:40:28,611] {logging_mixin.py:115} INFO - [2022-06-23 15:40:28,611] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:40:31,147] {logging_mixin.py:115} INFO - [2022-06-23 15:40:31,145] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:40:31,147] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:40:31,159] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.551 seconds
[2022-06-23 15:41:01,831] {processor.py:153} INFO - Started process (PID=3287) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:41:01,831] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:41:01,831] {logging_mixin.py:115} INFO - [2022-06-23 15:41:01,831] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:41:04,458] {logging_mixin.py:115} INFO - [2022-06-23 15:41:04,456] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:41:04,458] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:41:04,470] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.641 seconds
[2022-06-23 15:41:34,565] {processor.py:153} INFO - Started process (PID=3456) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:41:34,565] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:41:34,566] {logging_mixin.py:115} INFO - [2022-06-23 15:41:34,566] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:41:37,097] {logging_mixin.py:115} INFO - [2022-06-23 15:41:37,095] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:41:37,097] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:41:37,111] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.549 seconds
[2022-06-23 15:42:07,874] {processor.py:153} INFO - Started process (PID=3626) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:42:07,874] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:42:07,875] {logging_mixin.py:115} INFO - [2022-06-23 15:42:07,875] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:42:10,369] {logging_mixin.py:115} INFO - [2022-06-23 15:42:10,367] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:42:10,369] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:42:10,382] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.512 seconds
[2022-06-23 15:42:41,133] {processor.py:153} INFO - Started process (PID=3794) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:42:41,142] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:42:41,142] {logging_mixin.py:115} INFO - [2022-06-23 15:42:41,142] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:42:43,817] {logging_mixin.py:115} INFO - [2022-06-23 15:42:43,815] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:42:43,817] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:42:43,831] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.700 seconds
[2022-06-23 15:43:14,400] {processor.py:153} INFO - Started process (PID=3964) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:43:14,400] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:43:14,401] {logging_mixin.py:115} INFO - [2022-06-23 15:43:14,401] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:43:16,994] {logging_mixin.py:115} INFO - [2022-06-23 15:43:16,992] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:43:16,995] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:43:17,011] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.616 seconds
[2022-06-23 15:43:47,682] {processor.py:153} INFO - Started process (PID=4132) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:43:47,682] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:43:47,682] {logging_mixin.py:115} INFO - [2022-06-23 15:43:47,682] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:43:50,340] {logging_mixin.py:115} INFO - [2022-06-23 15:43:50,338] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:43:50,340] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:43:50,353] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.673 seconds
[2022-06-23 15:44:20,926] {processor.py:153} INFO - Started process (PID=4300) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:44:20,927] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:44:20,927] {logging_mixin.py:115} INFO - [2022-06-23 15:44:20,927] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:44:23,670] {logging_mixin.py:115} INFO - [2022-06-23 15:44:23,668] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:44:23,670] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:44:23,684] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.761 seconds
[2022-06-23 15:44:54,161] {processor.py:153} INFO - Started process (PID=4471) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:44:54,162] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:44:54,162] {logging_mixin.py:115} INFO - [2022-06-23 15:44:54,162] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:44:56,981] {logging_mixin.py:115} INFO - [2022-06-23 15:44:56,979] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:44:56,982] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:44:56,994] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.835 seconds
[2022-06-23 15:45:27,407] {processor.py:153} INFO - Started process (PID=4643) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:45:27,408] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:45:27,409] {logging_mixin.py:115} INFO - [2022-06-23 15:45:27,408] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:45:30,240] {logging_mixin.py:115} INFO - [2022-06-23 15:45:30,238] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:45:30,241] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:45:30,262] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.857 seconds
[2022-06-23 15:46:00,655] {processor.py:153} INFO - Started process (PID=4811) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:46:00,655] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:46:00,655] {logging_mixin.py:115} INFO - [2022-06-23 15:46:00,655] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:46:03,453] {logging_mixin.py:115} INFO - [2022-06-23 15:46:03,451] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:46:03,453] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:46:03,468] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.815 seconds
[2022-06-23 15:46:33,905] {processor.py:153} INFO - Started process (PID=4990) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:46:33,907] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:46:33,907] {logging_mixin.py:115} INFO - [2022-06-23 15:46:33,907] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:46:36,541] {logging_mixin.py:115} INFO - [2022-06-23 15:46:36,539] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:46:36,541] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:46:36,557] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.654 seconds
[2022-06-23 15:47:06,668] {processor.py:153} INFO - Started process (PID=5159) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:47:06,668] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:47:06,669] {logging_mixin.py:115} INFO - [2022-06-23 15:47:06,669] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:47:09,276] {logging_mixin.py:115} INFO - [2022-06-23 15:47:09,274] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:47:09,276] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:47:09,289] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.627 seconds
[2022-06-23 15:47:39,389] {processor.py:153} INFO - Started process (PID=5329) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:47:39,391] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:47:39,391] {logging_mixin.py:115} INFO - [2022-06-23 15:47:39,391] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:47:42,146] {logging_mixin.py:115} INFO - [2022-06-23 15:47:42,144] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:47:42,146] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:47:42,159] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.772 seconds
[2022-06-23 15:48:12,687] {processor.py:153} INFO - Started process (PID=5499) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:48:12,688] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:48:12,689] {logging_mixin.py:115} INFO - [2022-06-23 15:48:12,689] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:48:15,316] {logging_mixin.py:115} INFO - [2022-06-23 15:48:15,315] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:48:15,317] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:48:15,330] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.649 seconds
[2022-06-23 15:48:45,990] {processor.py:153} INFO - Started process (PID=5668) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:48:45,999] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:48:45,999] {logging_mixin.py:115} INFO - [2022-06-23 15:48:45,999] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:48:48,573] {logging_mixin.py:115} INFO - [2022-06-23 15:48:48,572] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:48:48,574] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:48:48,587] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.598 seconds
[2022-06-23 15:49:19,273] {processor.py:153} INFO - Started process (PID=5835) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:49:19,275] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:49:19,275] {logging_mixin.py:115} INFO - [2022-06-23 15:49:19,275] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:49:21,808] {logging_mixin.py:115} INFO - [2022-06-23 15:49:21,806] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:49:21,808] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:49:21,822] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.551 seconds
[2022-06-23 15:49:52,539] {processor.py:153} INFO - Started process (PID=6002) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:49:52,539] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:49:52,540] {logging_mixin.py:115} INFO - [2022-06-23 15:49:52,540] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:49:55,033] {logging_mixin.py:115} INFO - [2022-06-23 15:49:55,032] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:49:55,034] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:49:55,046] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.509 seconds
[2022-06-23 15:50:25,810] {processor.py:153} INFO - Started process (PID=6169) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:50:25,811] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:50:25,812] {logging_mixin.py:115} INFO - [2022-06-23 15:50:25,812] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:50:28,468] {logging_mixin.py:115} INFO - [2022-06-23 15:50:28,466] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:50:28,468] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:50:28,481] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.672 seconds
[2022-06-23 15:50:59,062] {processor.py:153} INFO - Started process (PID=6338) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:50:59,062] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:50:59,063] {logging_mixin.py:115} INFO - [2022-06-23 15:50:59,063] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:51:01,536] {logging_mixin.py:115} INFO - [2022-06-23 15:51:01,534] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:51:01,536] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:51:01,549] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.490 seconds
[2022-06-23 15:51:32,313] {processor.py:153} INFO - Started process (PID=6506) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:51:32,315] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:51:32,316] {logging_mixin.py:115} INFO - [2022-06-23 15:51:32,316] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:51:34,952] {logging_mixin.py:115} INFO - [2022-06-23 15:51:34,951] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:51:34,953] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:51:34,964] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.652 seconds
[2022-06-23 15:52:05,553] {processor.py:153} INFO - Started process (PID=6675) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:52:05,554] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:52:05,555] {logging_mixin.py:115} INFO - [2022-06-23 15:52:05,555] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:52:08,243] {logging_mixin.py:115} INFO - [2022-06-23 15:52:08,242] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:52:08,244] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:52:08,255] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.704 seconds
[2022-06-23 15:52:38,815] {processor.py:153} INFO - Started process (PID=6842) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:52:38,815] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:52:38,816] {logging_mixin.py:115} INFO - [2022-06-23 15:52:38,816] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:52:41,400] {logging_mixin.py:115} INFO - [2022-06-23 15:52:41,398] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:52:41,400] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:52:41,414] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.600 seconds
[2022-06-23 15:53:11,509] {processor.py:153} INFO - Started process (PID=7012) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:53:11,510] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:53:11,510] {logging_mixin.py:115} INFO - [2022-06-23 15:53:11,510] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:53:14,258] {logging_mixin.py:115} INFO - [2022-06-23 15:53:14,256] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:53:14,259] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:53:14,272] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.766 seconds
[2022-06-23 15:53:44,378] {processor.py:153} INFO - Started process (PID=7183) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:53:44,380] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:53:44,381] {logging_mixin.py:115} INFO - [2022-06-23 15:53:44,381] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:53:47,133] {logging_mixin.py:115} INFO - [2022-06-23 15:53:47,131] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:53:47,134] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:53:47,147] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.774 seconds
[2022-06-23 15:54:17,685] {processor.py:153} INFO - Started process (PID=7354) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:54:17,686] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:54:17,686] {logging_mixin.py:115} INFO - [2022-06-23 15:54:17,686] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:54:20,480] {logging_mixin.py:115} INFO - [2022-06-23 15:54:20,478] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:54:20,480] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:54:20,493] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.810 seconds
[2022-06-23 15:54:50,938] {processor.py:153} INFO - Started process (PID=7523) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:54:50,938] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:54:50,938] {logging_mixin.py:115} INFO - [2022-06-23 15:54:50,938] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:54:53,615] {logging_mixin.py:115} INFO - [2022-06-23 15:54:53,613] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:54:53,615] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:54:53,628] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.693 seconds
[2022-06-23 15:55:24,182] {processor.py:153} INFO - Started process (PID=7702) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:55:24,182] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:55:24,182] {logging_mixin.py:115} INFO - [2022-06-23 15:55:24,182] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:55:27,077] {logging_mixin.py:115} INFO - [2022-06-23 15:55:27,076] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:55:27,078] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:55:27,089] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.910 seconds
[2022-06-23 15:55:57,416] {processor.py:153} INFO - Started process (PID=7871) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:55:57,417] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:55:57,418] {logging_mixin.py:115} INFO - [2022-06-23 15:55:57,418] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:56:00,063] {logging_mixin.py:115} INFO - [2022-06-23 15:56:00,062] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:56:00,064] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:56:00,076] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.662 seconds
[2022-06-23 15:56:30,703] {processor.py:153} INFO - Started process (PID=8042) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:56:30,703] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:56:30,704] {logging_mixin.py:115} INFO - [2022-06-23 15:56:30,704] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:56:33,322] {logging_mixin.py:115} INFO - [2022-06-23 15:56:33,319] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:56:33,322] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:56:33,337] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.640 seconds
[2022-06-23 15:57:03,969] {processor.py:153} INFO - Started process (PID=8210) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:57:03,970] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:57:03,970] {logging_mixin.py:115} INFO - [2022-06-23 15:57:03,970] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:57:06,706] {logging_mixin.py:115} INFO - [2022-06-23 15:57:06,705] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:57:06,707] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:57:06,719] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.751 seconds
[2022-06-23 15:58:15,724] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:58:15,725] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:58:15,725] {logging_mixin.py:115} INFO - [2022-06-23 15:58:15,725] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:58:22,360] {logging_mixin.py:115} INFO - [2022-06-23 15:58:22,357] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:58:22,361] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:58:22,380] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 6.659 seconds
[2022-06-23 15:58:52,426] {processor.py:153} INFO - Started process (PID=216) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:58:52,428] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:58:52,428] {logging_mixin.py:115} INFO - [2022-06-23 15:58:52,428] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:58:55,185] {logging_mixin.py:115} INFO - [2022-06-23 15:58:55,184] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:58:55,185] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:58:55,197] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.773 seconds
[2022-06-23 15:59:25,533] {processor.py:153} INFO - Started process (PID=393) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:59:25,533] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:59:25,533] {logging_mixin.py:115} INFO - [2022-06-23 15:59:25,533] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:59:28,260] {logging_mixin.py:115} INFO - [2022-06-23 15:59:28,258] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 15:59:28,260] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:59:28,273] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.741 seconds
[2022-06-23 15:59:58,791] {processor.py:153} INFO - Started process (PID=561) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 15:59:58,791] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 15:59:58,791] {logging_mixin.py:115} INFO - [2022-06-23 15:59:58,791] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:00:01,477] {logging_mixin.py:115} INFO - [2022-06-23 16:00:01,475] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:00:01,477] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:00:01,490] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.701 seconds
[2022-06-23 16:00:32,028] {processor.py:153} INFO - Started process (PID=730) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:00:32,028] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:00:32,029] {logging_mixin.py:115} INFO - [2022-06-23 16:00:32,029] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:00:34,887] {logging_mixin.py:115} INFO - [2022-06-23 16:00:34,885] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:00:34,887] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:00:34,898] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.872 seconds
[2022-06-23 16:01:05,274] {processor.py:153} INFO - Started process (PID=899) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:01:05,276] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:01:05,276] {logging_mixin.py:115} INFO - [2022-06-23 16:01:05,276] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:01:08,247] {logging_mixin.py:115} INFO - [2022-06-23 16:01:08,245] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:01:08,247] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:01:08,261] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.988 seconds
[2022-06-23 16:01:38,390] {processor.py:153} INFO - Started process (PID=1072) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:01:38,392] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:01:38,392] {logging_mixin.py:115} INFO - [2022-06-23 16:01:38,392] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:01:41,017] {logging_mixin.py:115} INFO - [2022-06-23 16:01:41,015] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:01:41,018] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:01:41,032] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.644 seconds
[2022-06-23 16:02:11,626] {processor.py:153} INFO - Started process (PID=1241) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:02:11,627] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:02:11,628] {logging_mixin.py:115} INFO - [2022-06-23 16:02:11,628] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:02:14,137] {logging_mixin.py:115} INFO - [2022-06-23 16:02:14,136] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:02:14,138] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:02:14,152] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.527 seconds
[2022-06-23 16:02:44,865] {processor.py:153} INFO - Started process (PID=1409) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:02:44,865] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:02:44,865] {logging_mixin.py:115} INFO - [2022-06-23 16:02:44,865] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:02:47,519] {logging_mixin.py:115} INFO - [2022-06-23 16:02:47,517] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:02:47,519] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:02:47,533] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.670 seconds
[2022-06-23 16:03:18,127] {processor.py:153} INFO - Started process (PID=1577) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:03:18,127] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:03:18,127] {logging_mixin.py:115} INFO - [2022-06-23 16:03:18,127] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:03:20,820] {logging_mixin.py:115} INFO - [2022-06-23 16:03:20,817] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:03:20,820] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:03:20,832] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.707 seconds
[2022-06-23 16:03:51,474] {processor.py:153} INFO - Started process (PID=1748) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:03:51,476] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:03:51,476] {logging_mixin.py:115} INFO - [2022-06-23 16:03:51,476] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:03:54,050] {logging_mixin.py:115} INFO - [2022-06-23 16:03:54,048] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:03:54,050] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:03:54,096] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.624 seconds
[2022-06-23 16:04:24,705] {processor.py:153} INFO - Started process (PID=1916) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:04:24,706] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:04:24,707] {logging_mixin.py:115} INFO - [2022-06-23 16:04:24,707] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:04:27,427] {logging_mixin.py:115} INFO - [2022-06-23 16:04:27,425] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:04:27,427] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:04:27,439] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.737 seconds
[2022-06-23 16:04:57,950] {processor.py:153} INFO - Started process (PID=2086) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:04:57,952] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:04:57,953] {logging_mixin.py:115} INFO - [2022-06-23 16:04:57,953] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:05:00,579] {logging_mixin.py:115} INFO - [2022-06-23 16:05:00,577] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:05:00,579] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:05:00,592] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.646 seconds
[2022-06-23 16:05:31,200] {processor.py:153} INFO - Started process (PID=2255) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:05:31,200] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:05:31,200] {logging_mixin.py:115} INFO - [2022-06-23 16:05:31,200] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:05:33,869] {logging_mixin.py:115} INFO - [2022-06-23 16:05:33,867] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:05:33,869] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:05:33,883] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.685 seconds
[2022-06-23 16:06:04,469] {processor.py:153} INFO - Started process (PID=2427) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:06:04,471] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:06:04,472] {logging_mixin.py:115} INFO - [2022-06-23 16:06:04,471] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:06:07,244] {logging_mixin.py:115} INFO - [2022-06-23 16:06:07,242] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:06:07,244] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:06:07,257] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.792 seconds
[2022-06-23 16:06:37,726] {processor.py:153} INFO - Started process (PID=2594) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:06:37,727] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:06:37,727] {logging_mixin.py:115} INFO - [2022-06-23 16:06:37,727] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:06:40,413] {logging_mixin.py:115} INFO - [2022-06-23 16:06:40,412] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:06:40,414] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:06:40,426] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.701 seconds
[2022-06-23 16:07:10,958] {processor.py:153} INFO - Started process (PID=2761) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:07:10,959] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:07:10,960] {logging_mixin.py:115} INFO - [2022-06-23 16:07:10,960] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:07:13,739] {logging_mixin.py:115} INFO - [2022-06-23 16:07:13,738] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:07:13,740] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:07:13,753] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.798 seconds
[2022-06-23 16:07:44,217] {processor.py:153} INFO - Started process (PID=2939) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:07:44,217] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:07:44,217] {logging_mixin.py:115} INFO - [2022-06-23 16:07:44,217] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:07:46,841] {logging_mixin.py:115} INFO - [2022-06-23 16:07:46,839] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:07:46,841] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:07:46,854] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.640 seconds
[2022-06-23 16:08:17,482] {processor.py:153} INFO - Started process (PID=3109) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:08:17,483] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:08:17,484] {logging_mixin.py:115} INFO - [2022-06-23 16:08:17,483] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:08:20,099] {logging_mixin.py:115} INFO - [2022-06-23 16:08:20,097] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 12, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 37, in <module>
    .config(conf=sc.getConf()) \
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 272, in getOrCreate
    session = SparkSession(sc, options=self._options)
  File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/sql/session.py", line 307, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1586, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 332, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:
py4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)
	at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)
	at py4j.Gateway.invoke(Gateway.java:237)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2022-06-23 16:08:20,099] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:08:20,113] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 2.637 seconds
[2022-06-23 16:28:59,597] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:28:59,597] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:28:59,598] {logging_mixin.py:115} INFO - [2022-06-23 16:28:59,598] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:28:59,602] {logging_mixin.py:115} INFO - [2022-06-23 16:28:59,600] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.operators.spark import SparkSubmitOperator
ModuleNotFoundError: No module named 'airflow.operators.spark'
[2022-06-23 16:28:59,602] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:28:59,626] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.032 seconds
[2022-06-23 16:29:29,755] {processor.py:153} INFO - Started process (PID=68) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:29:29,756] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:29:29,756] {logging_mixin.py:115} INFO - [2022-06-23 16:29:29,756] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:29:29,758] {logging_mixin.py:115} INFO - [2022-06-23 16:29:29,757] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.operators.spark import SparkSubmitOperator
ModuleNotFoundError: No module named 'airflow.operators.spark'
[2022-06-23 16:29:29,758] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:29:29,773] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.021 seconds
[2022-06-23 16:29:57,843] {processor.py:153} INFO - Started process (PID=90) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:29:57,843] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:29:57,843] {logging_mixin.py:115} INFO - [2022-06-23 16:29:57,843] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:29:57,845] {logging_mixin.py:115} INFO - [2022-06-23 16:29:57,845] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.operators.spark_submit_operator import SparkSubmitOperator
ModuleNotFoundError: No module named 'airflow.operators.spark_submit_operator'
[2022-06-23 16:29:57,845] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:29:57,858] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.017 seconds
[2022-06-23 16:30:39,060] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:30:39,062] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:30:39,063] {logging_mixin.py:115} INFO - [2022-06-23 16:30:39,063] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:30:39,065] {logging_mixin.py:115} INFO - [2022-06-23 16:30:39,064] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.operators.spark_submit_operator import SparkSubmitOperator
ModuleNotFoundError: No module named 'airflow.operators.spark_submit_operator'
[2022-06-23 16:30:39,065] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:30:39,089] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.030 seconds
[2022-06-23 16:31:09,201] {processor.py:153} INFO - Started process (PID=69) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:31:09,201] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:31:09,202] {logging_mixin.py:115} INFO - [2022-06-23 16:31:09,202] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:31:09,203] {logging_mixin.py:115} INFO - [2022-06-23 16:31:09,203] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.operators.spark_submit_operator import SparkSubmitOperator
ModuleNotFoundError: No module named 'airflow.operators.spark_submit_operator'
[2022-06-23 16:31:09,203] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:31:09,218] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.019 seconds
[2022-06-23 16:31:28,286] {processor.py:153} INFO - Started process (PID=92) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:31:28,287] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:31:28,288] {logging_mixin.py:115} INFO - [2022-06-23 16:31:28,288] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:31:28,293] {logging_mixin.py:115} INFO - [2022-06-23 16:31:28,292] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 22, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator  # noqa
ModuleNotFoundError: No module named 'airflow.providers.apache'
[2022-06-23 16:31:28,294] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:31:28,308] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.027 seconds
[2022-06-23 16:32:11,741] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:32:11,742] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:32:11,743] {logging_mixin.py:115} INFO - [2022-06-23 16:32:11,742] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:32:11,748] {logging_mixin.py:115} INFO - [2022-06-23 16:32:11,747] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 22, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator  # noqa
ModuleNotFoundError: No module named 'airflow.providers.apache'
[2022-06-23 16:32:11,748] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:32:11,772] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.034 seconds
[2022-06-23 16:32:41,909] {processor.py:153} INFO - Started process (PID=71) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:32:41,909] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:32:41,910] {logging_mixin.py:115} INFO - [2022-06-23 16:32:41,910] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:32:41,913] {logging_mixin.py:115} INFO - [2022-06-23 16:32:41,912] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 22, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator  # noqa
ModuleNotFoundError: No module named 'airflow.providers.apache'
[2022-06-23 16:32:41,913] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:32:41,930] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.025 seconds
[2022-06-23 16:33:12,009] {processor.py:153} INFO - Started process (PID=103) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:33:12,010] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:33:12,011] {logging_mixin.py:115} INFO - [2022-06-23 16:33:12,011] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:33:12,014] {logging_mixin.py:115} INFO - [2022-06-23 16:33:12,013] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 22, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator  # noqa
ModuleNotFoundError: No module named 'airflow.providers.apache'
[2022-06-23 16:33:12,014] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:33:12,029] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.022 seconds
[2022-06-23 16:33:42,119] {processor.py:153} INFO - Started process (PID=134) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:33:42,120] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:33:42,120] {logging_mixin.py:115} INFO - [2022-06-23 16:33:42,120] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:33:42,124] {logging_mixin.py:115} INFO - [2022-06-23 16:33:42,123] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 22, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator  # noqa
ModuleNotFoundError: No module named 'airflow.providers.apache'
[2022-06-23 16:33:42,124] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:33:42,142] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.027 seconds
[2022-06-23 16:34:07,426] {processor.py:153} INFO - Started process (PID=156) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:34:07,427] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:34:07,428] {logging_mixin.py:115} INFO - [2022-06-23 16:34:07,427] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:34:07,433] {logging_mixin.py:115} INFO - [2022-06-23 16:34:07,432] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
ModuleNotFoundError: No module named 'airflow.providers.apache'
[2022-06-23 16:34:07,433] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:34:07,459] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.038 seconds
[2022-06-23 16:34:08,437] {processor.py:153} INFO - Started process (PID=157) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:34:08,437] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:34:08,438] {logging_mixin.py:115} INFO - [2022-06-23 16:34:08,438] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:34:08,439] {logging_mixin.py:115} INFO - [2022-06-23 16:34:08,439] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
ModuleNotFoundError: No module named 'airflow.providers.apache'
[2022-06-23 16:34:08,439] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:34:08,452] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.018 seconds
[2022-06-23 16:36:55,026] {processor.py:153} INFO - Started process (PID=48) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:36:55,027] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:36:55,027] {logging_mixin.py:115} INFO - [2022-06-23 16:36:55,027] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:37:02,573] {logging_mixin.py:115} INFO - [2022-06-23 16:37:02,572] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 85, in <module>
    parquet_file = sys.argv[2]
IndexError: list index out of range
[2022-06-23 16:37:02,574] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:37:02,598] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.576 seconds
[2022-06-23 16:37:33,127] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:37:33,136] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:37:33,137] {logging_mixin.py:115} INFO - [2022-06-23 16:37:33,137] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:37:36,193] {logging_mixin.py:115} INFO - [2022-06-23 16:37:36,192] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 85, in <module>
    parquet_file = sys.argv[2]
IndexError: list index out of range
[2022-06-23 16:37:36,193] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:37:36,207] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.085 seconds
[2022-06-23 16:38:06,275] {processor.py:153} INFO - Started process (PID=394) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:38:06,276] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:38:06,277] {logging_mixin.py:115} INFO - [2022-06-23 16:38:06,276] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:38:09,536] {logging_mixin.py:115} INFO - [2022-06-23 16:38:09,535] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 85, in <module>
    parquet_file = sys.argv[2]
IndexError: list index out of range
[2022-06-23 16:38:09,536] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:38:09,559] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.286 seconds
[2022-06-23 16:38:43,441] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:38:43,442] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:38:43,443] {logging_mixin.py:115} INFO - [2022-06-23 16:38:43,443] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:38:57,959] {logging_mixin.py:115} INFO - [2022-06-23 16:38:57,909] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (86c5d6f6cfd7 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:38:58,007] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:38:58,026] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 14.588 seconds
[2022-06-23 16:39:46,207] {processor.py:153} INFO - Started process (PID=39) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:39:46,208] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:39:46,208] {logging_mixin.py:115} INFO - [2022-06-23 16:39:46,208] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:40:00,123] {logging_mixin.py:115} INFO - [2022-06-23 16:40:00,074] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:40:00,167] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:40:00,187] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 13.983 seconds
[2022-06-23 16:40:30,925] {processor.py:153} INFO - Started process (PID=244) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:40:30,926] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:40:30,927] {logging_mixin.py:115} INFO - [2022-06-23 16:40:30,926] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:40:38,247] {logging_mixin.py:115} INFO - [2022-06-23 16:40:38,205] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:40:38,291] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:40:38,330] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.410 seconds
[2022-06-23 16:41:09,180] {processor.py:153} INFO - Started process (PID=439) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:41:09,181] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:41:09,182] {logging_mixin.py:115} INFO - [2022-06-23 16:41:09,181] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:41:16,323] {logging_mixin.py:115} INFO - [2022-06-23 16:41:16,276] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:41:16,367] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:41:16,385] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.210 seconds
[2022-06-23 16:41:46,466] {processor.py:153} INFO - Started process (PID=621) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:41:46,466] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:41:46,466] {logging_mixin.py:115} INFO - [2022-06-23 16:41:46,466] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:41:54,779] {logging_mixin.py:115} INFO - [2022-06-23 16:41:54,734] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:41:54,823] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:41:54,843] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.379 seconds
[2022-06-23 16:42:24,919] {processor.py:153} INFO - Started process (PID=816) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:42:24,921] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:42:24,922] {logging_mixin.py:115} INFO - [2022-06-23 16:42:24,922] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:42:32,338] {logging_mixin.py:115} INFO - [2022-06-23 16:42:32,292] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:42:32,387] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:42:32,402] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.488 seconds
[2022-06-23 16:43:03,116] {processor.py:153} INFO - Started process (PID=999) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:43:03,117] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:43:03,117] {logging_mixin.py:115} INFO - [2022-06-23 16:43:03,117] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:43:10,615] {logging_mixin.py:115} INFO - [2022-06-23 16:43:10,567] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:43:10,659] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:43:10,674] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.561 seconds
[2022-06-23 16:43:41,355] {processor.py:153} INFO - Started process (PID=1191) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:43:41,357] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:43:41,358] {logging_mixin.py:115} INFO - [2022-06-23 16:43:41,358] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:43:48,690] {logging_mixin.py:115} INFO - [2022-06-23 16:43:48,641] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:43:48,734] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:43:48,747] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.394 seconds
[2022-06-23 16:44:18,804] {processor.py:153} INFO - Started process (PID=1375) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:44:18,804] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:44:18,804] {logging_mixin.py:115} INFO - [2022-06-23 16:44:18,804] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:44:26,203] {logging_mixin.py:115} INFO - [2022-06-23 16:44:26,160] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:44:26,251] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:44:26,266] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.464 seconds
[2022-06-23 16:44:57,097] {processor.py:153} INFO - Started process (PID=1572) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:44:57,097] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:44:57,097] {logging_mixin.py:115} INFO - [2022-06-23 16:44:57,097] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:45:04,171] {logging_mixin.py:115} INFO - [2022-06-23 16:45:04,125] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:45:04,215] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:45:04,228] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.133 seconds
[2022-06-23 16:45:34,312] {processor.py:153} INFO - Started process (PID=1756) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:45:34,313] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:45:34,313] {logging_mixin.py:115} INFO - [2022-06-23 16:45:34,313] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:45:41,471] {logging_mixin.py:115} INFO - [2022-06-23 16:45:41,427] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:45:41,515] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:45:41,532] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.221 seconds
[2022-06-23 16:46:11,603] {processor.py:153} INFO - Started process (PID=1950) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:46:11,613] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:46:11,614] {logging_mixin.py:115} INFO - [2022-06-23 16:46:11,614] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:46:18,539] {logging_mixin.py:115} INFO - [2022-06-23 16:46:18,495] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:46:18,583] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:46:18,615] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.014 seconds
[2022-06-23 16:46:48,702] {processor.py:153} INFO - Started process (PID=2133) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:46:48,703] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:46:48,703] {logging_mixin.py:115} INFO - [2022-06-23 16:46:48,703] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:46:55,951] {logging_mixin.py:115} INFO - [2022-06-23 16:46:55,907] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:46:55,995] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:46:56,009] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.309 seconds
[2022-06-23 16:47:26,082] {processor.py:153} INFO - Started process (PID=2327) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:47:26,084] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:47:26,085] {logging_mixin.py:115} INFO - [2022-06-23 16:47:26,085] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:47:33,531] {logging_mixin.py:115} INFO - [2022-06-23 16:47:33,487] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:47:33,575] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:47:33,587] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.509 seconds
[2022-06-23 16:48:04,186] {processor.py:153} INFO - Started process (PID=2511) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:48:04,186] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:48:04,187] {logging_mixin.py:115} INFO - [2022-06-23 16:48:04,187] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:48:11,418] {logging_mixin.py:115} INFO - [2022-06-23 16:48:11,375] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:48:11,463] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:48:11,479] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.299 seconds
[2022-06-23 16:48:41,552] {processor.py:153} INFO - Started process (PID=2703) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:48:41,554] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:48:41,554] {logging_mixin.py:115} INFO - [2022-06-23 16:48:41,554] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:48:49,083] {logging_mixin.py:115} INFO - [2022-06-23 16:48:49,039] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:48:49,127] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:48:49,141] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.590 seconds
[2022-06-23 16:49:19,700] {processor.py:153} INFO - Started process (PID=2885) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:49:19,700] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:49:19,700] {logging_mixin.py:115} INFO - [2022-06-23 16:49:19,700] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:49:26,950] {logging_mixin.py:115} INFO - [2022-06-23 16:49:26,903] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:49:26,999] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:49:27,027] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.331 seconds
[2022-06-23 16:49:57,117] {processor.py:153} INFO - Started process (PID=3071) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:49:57,117] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:49:57,118] {logging_mixin.py:115} INFO - [2022-06-23 16:49:57,118] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:50:04,223] {logging_mixin.py:115} INFO - [2022-06-23 16:50:04,177] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:50:04,267] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:50:04,280] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.167 seconds
[2022-06-23 16:50:34,360] {processor.py:153} INFO - Started process (PID=3262) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:50:34,361] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:50:34,361] {logging_mixin.py:115} INFO - [2022-06-23 16:50:34,361] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:50:41,879] {logging_mixin.py:115} INFO - [2022-06-23 16:50:41,836] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:50:41,923] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:50:41,946] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.588 seconds
[2022-06-23 16:51:12,500] {processor.py:153} INFO - Started process (PID=3444) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:51:12,502] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:51:12,503] {logging_mixin.py:115} INFO - [2022-06-23 16:51:12,502] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:51:19,659] {logging_mixin.py:115} INFO - [2022-06-23 16:51:19,615] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:51:19,707] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:51:19,721] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.227 seconds
[2022-06-23 16:51:49,806] {processor.py:153} INFO - Started process (PID=3636) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:51:49,808] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:51:49,809] {logging_mixin.py:115} INFO - [2022-06-23 16:51:49,809] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:51:57,115] {logging_mixin.py:115} INFO - [2022-06-23 16:51:57,069] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:51:57,158] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:51:57,173] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.368 seconds
[2022-06-23 16:52:27,245] {processor.py:153} INFO - Started process (PID=3825) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:52:27,245] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:52:27,246] {logging_mixin.py:115} INFO - [2022-06-23 16:52:27,246] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:52:34,523] {logging_mixin.py:115} INFO - [2022-06-23 16:52:34,480] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:52:34,571] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:52:34,599] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.359 seconds
[2022-06-23 16:53:04,638] {processor.py:153} INFO - Started process (PID=4014) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:53:04,647] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:53:04,648] {logging_mixin.py:115} INFO - [2022-06-23 16:53:04,648] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:53:12,047] {logging_mixin.py:115} INFO - [2022-06-23 16:53:12,003] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:53:12,091] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:53:12,105] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.472 seconds
[2022-06-23 16:53:42,714] {processor.py:153} INFO - Started process (PID=4196) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:53:42,714] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:53:42,715] {logging_mixin.py:115} INFO - [2022-06-23 16:53:42,715] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:53:49,955] {logging_mixin.py:115} INFO - [2022-06-23 16:53:49,911] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:53:49,999] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:53:50,033] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.322 seconds
[2022-06-23 16:54:20,129] {processor.py:153} INFO - Started process (PID=4390) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:54:20,131] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:54:20,132] {logging_mixin.py:115} INFO - [2022-06-23 16:54:20,132] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:54:27,587] {logging_mixin.py:115} INFO - [2022-06-23 16:54:27,544] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:54:27,635] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:54:27,648] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.525 seconds
[2022-06-23 16:54:58,412] {processor.py:153} INFO - Started process (PID=4573) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:54:58,414] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:54:58,415] {logging_mixin.py:115} INFO - [2022-06-23 16:54:58,414] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:55:06,259] {logging_mixin.py:115} INFO - [2022-06-23 16:55:06,215] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:55:06,303] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:55:06,333] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.923 seconds
[2022-06-23 16:55:36,665] {processor.py:153} INFO - Started process (PID=4768) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:55:36,666] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:55:36,667] {logging_mixin.py:115} INFO - [2022-06-23 16:55:36,667] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:55:44,467] {logging_mixin.py:115} INFO - [2022-06-23 16:55:44,422] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:55:44,516] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:55:44,545] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 7.881 seconds
[2022-06-23 16:56:14,910] {processor.py:153} INFO - Started process (PID=4950) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:56:14,912] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:56:14,912] {logging_mixin.py:115} INFO - [2022-06-23 16:56:14,912] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:56:29,783] {logging_mixin.py:115} INFO - [2022-06-23 16:56:29,736] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:56:29,827] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:56:29,849] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 14.942 seconds
[2022-06-23 16:57:00,025] {processor.py:153} INFO - Started process (PID=5155) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:57:00,029] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:57:00,035] {logging_mixin.py:115} INFO - [2022-06-23 16:57:00,035] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:57:20,650] {logging_mixin.py:115} INFO - [2022-06-23 16:57:20,601] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:57:20,695] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:57:20,716] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 20.707 seconds
[2022-06-23 16:57:51,207] {processor.py:153} INFO - Started process (PID=5360) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:57:51,208] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:57:51,208] {logging_mixin.py:115} INFO - [2022-06-23 16:57:51,208] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:58:21,213] {logging_mixin.py:115} INFO - [2022-06-23 16:58:21,210] {timeout.py:67} ERROR - Process timed out, PID: 5360
[2022-06-23 16:58:21,226] {logging_mixin.py:115} INFO - [2022-06-23 16:58:21,213] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 5360
[2022-06-23 16:58:21,227] {logging_mixin.py:115} INFO - [2022-06-23 16:58:21,227] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 16:58:21,238] {logging_mixin.py:115} INFO - [2022-06-23 16:58:21,228] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 5360

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 16:58:21,246] {logging_mixin.py:115} INFO - [2022-06-23 16:58:21,246] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 16:58:21,286] {logging_mixin.py:115} INFO - [2022-06-23 16:58:21,275] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o44.collectToPython
[2022-06-23 16:58:21,289] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:58:21,463] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.258 seconds
[2022-06-23 16:58:51,697] {processor.py:153} INFO - Started process (PID=5575) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:58:51,700] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:58:51,700] {logging_mixin.py:115} INFO - [2022-06-23 16:58:51,700] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:59:06,855] {logging_mixin.py:115} INFO - [2022-06-23 16:59:06,811] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:59:06,899] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:59:06,918] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 15.224 seconds
[2022-06-23 16:59:37,080] {processor.py:153} INFO - Started process (PID=5770) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:59:37,083] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 16:59:37,085] {logging_mixin.py:115} INFO - [2022-06-23 16:59:37,085] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:59:56,199] {logging_mixin.py:115} INFO - [2022-06-23 16:59:56,148] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 16:59:56,243] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 16:59:56,264] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 19.193 seconds
[2022-06-23 17:00:26,345] {processor.py:153} INFO - Started process (PID=5977) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:00:26,346] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:00:26,346] {logging_mixin.py:115} INFO - [2022-06-23 17:00:26,346] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:00:56,351] {logging_mixin.py:115} INFO - [2022-06-23 17:00:56,348] {timeout.py:67} ERROR - Process timed out, PID: 5977
[2022-06-23 17:00:56,355] {logging_mixin.py:115} INFO - [2022-06-23 17:00:56,351] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 5977
[2022-06-23 17:00:56,356] {logging_mixin.py:115} INFO - [2022-06-23 17:00:56,355] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:00:56,360] {logging_mixin.py:115} INFO - [2022-06-23 17:00:56,356] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 5977

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:00:56,360] {logging_mixin.py:115} INFO - [2022-06-23 17:00:56,360] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:00:56,377] {logging_mixin.py:115} INFO - [2022-06-23 17:00:56,374] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o44.collectToPython
[2022-06-23 17:00:56,378] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:00:56,480] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.138 seconds
[2022-06-23 17:01:26,671] {processor.py:153} INFO - Started process (PID=6191) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:01:26,671] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:01:26,672] {logging_mixin.py:115} INFO - [2022-06-23 17:01:26,672] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:01:39,091] {logging_mixin.py:115} INFO - [2022-06-23 17:01:39,046] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:01:39,135] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:01:39,154] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 12.485 seconds
[2022-06-23 17:02:09,265] {processor.py:153} INFO - Started process (PID=6384) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:02:09,275] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:02:09,277] {logging_mixin.py:115} INFO - [2022-06-23 17:02:09,277] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:02:37,243] {logging_mixin.py:115} INFO - [2022-06-23 17:02:37,195] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:02:37,291] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:02:37,310] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 28.054 seconds
[2022-06-23 17:03:07,506] {processor.py:153} INFO - Started process (PID=6602) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:03:07,508] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:03:07,509] {logging_mixin.py:115} INFO - [2022-06-23 17:03:07,509] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:03:37,516] {logging_mixin.py:115} INFO - [2022-06-23 17:03:37,510] {timeout.py:67} ERROR - Process timed out, PID: 6602
[2022-06-23 17:03:37,524] {logging_mixin.py:115} INFO - [2022-06-23 17:03:37,518] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 6602
[2022-06-23 17:03:37,525] {logging_mixin.py:115} INFO - [2022-06-23 17:03:37,524] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:03:37,529] {logging_mixin.py:115} INFO - [2022-06-23 17:03:37,525] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 6602

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:03:37,530] {logging_mixin.py:115} INFO - [2022-06-23 17:03:37,530] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:03:37,547] {logging_mixin.py:115} INFO - [2022-06-23 17:03:37,545] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o44.collectToPython
[2022-06-23 17:03:37,549] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:03:37,642] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.138 seconds
[2022-06-23 17:04:08,304] {processor.py:153} INFO - Started process (PID=6810) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:04:08,304] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:04:08,304] {logging_mixin.py:115} INFO - [2022-06-23 17:04:08,304] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:04:22,091] {logging_mixin.py:115} INFO - [2022-06-23 17:04:22,044] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (904b9bdefaea executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:04:22,135] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:04:22,156] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 13.855 seconds
[2022-06-23 17:05:37,112] {processor.py:153} INFO - Started process (PID=47) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:05:37,114] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:05:37,115] {logging_mixin.py:115} INFO - [2022-06-23 17:05:37,115] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:05:58,671] {logging_mixin.py:115} INFO - [2022-06-23 17:05:58,620] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:05:58,719] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:05:58,786] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 21.677 seconds
[2022-06-23 17:06:28,923] {processor.py:153} INFO - Started process (PID=250) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:06:28,923] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:06:28,924] {logging_mixin.py:115} INFO - [2022-06-23 17:06:28,924] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:06:41,415] {logging_mixin.py:115} INFO - [2022-06-23 17:06:41,367] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:06:41,459] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:06:41,477] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 12.555 seconds
[2022-06-23 17:07:11,539] {processor.py:153} INFO - Started process (PID=450) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:07:11,540] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:07:11,541] {logging_mixin.py:115} INFO - [2022-06-23 17:07:11,541] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:07:41,543] {logging_mixin.py:115} INFO - [2022-06-23 17:07:41,542] {timeout.py:67} ERROR - Process timed out, PID: 450
[2022-06-23 17:07:41,545] {logging_mixin.py:115} INFO - [2022-06-23 17:07:41,544] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 450
[2022-06-23 17:07:41,546] {logging_mixin.py:115} INFO - [2022-06-23 17:07:41,546] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:07:41,548] {logging_mixin.py:115} INFO - [2022-06-23 17:07:41,546] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 450

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:07:41,548] {logging_mixin.py:115} INFO - [2022-06-23 17:07:41,548] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:07:41,551] {logging_mixin.py:115} INFO - [2022-06-23 17:07:41,549] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 76, in process_food_waste_data
    .parquet(f"gs://{BUCKET}/{raw_folder}/{parquet_file}")
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py", line 301, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o37.parquet
[2022-06-23 17:07:41,552] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:07:41,596] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.060 seconds
[2022-06-23 17:08:11,838] {processor.py:153} INFO - Started process (PID=670) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:08:11,840] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:08:11,840] {logging_mixin.py:115} INFO - [2022-06-23 17:08:11,840] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:08:24,631] {logging_mixin.py:115} INFO - [2022-06-23 17:08:24,582] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:08:24,675] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:08:24,706] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 12.870 seconds
[2022-06-23 17:08:55,034] {processor.py:153} INFO - Started process (PID=862) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:08:55,044] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:08:55,045] {logging_mixin.py:115} INFO - [2022-06-23 17:08:55,045] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:09:11,739] {logging_mixin.py:115} INFO - [2022-06-23 17:09:11,691] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:09:11,783] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:09:11,807] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 16.780 seconds
[2022-06-23 17:09:41,896] {processor.py:153} INFO - Started process (PID=1062) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:09:41,896] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:09:41,897] {logging_mixin.py:115} INFO - [2022-06-23 17:09:41,897] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:10:11,900] {logging_mixin.py:115} INFO - [2022-06-23 17:10:11,898] {timeout.py:67} ERROR - Process timed out, PID: 1062
[2022-06-23 17:10:11,903] {logging_mixin.py:115} INFO - [2022-06-23 17:10:11,900] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 1062
[2022-06-23 17:10:11,903] {logging_mixin.py:115} INFO - [2022-06-23 17:10:11,903] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:10:11,906] {logging_mixin.py:115} INFO - [2022-06-23 17:10:11,904] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 1062

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:10:11,906] {logging_mixin.py:115} INFO - [2022-06-23 17:10:11,906] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:10:11,909] {logging_mixin.py:115} INFO - [2022-06-23 17:10:11,907] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 76, in process_food_waste_data
    .parquet(f"gs://{BUCKET}/{raw_folder}/{parquet_file}")
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py", line 301, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o37.parquet
[2022-06-23 17:10:11,910] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:10:11,961] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.068 seconds
[2022-06-23 17:10:42,065] {processor.py:153} INFO - Started process (PID=1286) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:10:42,065] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:10:42,066] {logging_mixin.py:115} INFO - [2022-06-23 17:10:42,066] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:10:55,231] {logging_mixin.py:115} INFO - [2022-06-23 17:10:55,185] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:10:55,275] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:10:55,302] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 13.240 seconds
[2022-06-23 17:11:25,408] {processor.py:153} INFO - Started process (PID=1480) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:11:25,409] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:11:25,410] {logging_mixin.py:115} INFO - [2022-06-23 17:11:25,410] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:11:39,367] {logging_mixin.py:115} INFO - [2022-06-23 17:11:39,320] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:11:39,411] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:11:39,429] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 14.028 seconds
[2022-06-23 17:12:09,546] {processor.py:153} INFO - Started process (PID=1685) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:12:09,552] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:12:09,557] {logging_mixin.py:115} INFO - [2022-06-23 17:12:09,555] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:12:39,562] {logging_mixin.py:115} INFO - [2022-06-23 17:12:39,561] {timeout.py:67} ERROR - Process timed out, PID: 1685
[2022-06-23 17:12:39,565] {logging_mixin.py:115} INFO - [2022-06-23 17:12:39,563] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 1685
[2022-06-23 17:12:39,566] {logging_mixin.py:115} INFO - [2022-06-23 17:12:39,565] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:12:39,568] {logging_mixin.py:115} INFO - [2022-06-23 17:12:39,566] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 1685

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:12:39,568] {logging_mixin.py:115} INFO - [2022-06-23 17:12:39,568] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:12:39,571] {logging_mixin.py:115} INFO - [2022-06-23 17:12:39,569] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 76, in process_food_waste_data
    .parquet(f"gs://{BUCKET}/{raw_folder}/{parquet_file}")
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py", line 301, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o37.parquet
[2022-06-23 17:12:39,573] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:12:39,632] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.099 seconds
[2022-06-23 17:13:10,256] {processor.py:153} INFO - Started process (PID=1892) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:13:10,257] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:13:10,258] {logging_mixin.py:115} INFO - [2022-06-23 17:13:10,258] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:13:22,323] {logging_mixin.py:115} INFO - [2022-06-23 17:13:22,278] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:13:22,367] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:13:22,395] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 12.144 seconds
[2022-06-23 17:13:52,837] {processor.py:153} INFO - Started process (PID=2087) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:13:52,839] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:13:52,840] {logging_mixin.py:115} INFO - [2022-06-23 17:13:52,840] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:14:05,635] {logging_mixin.py:115} INFO - [2022-06-23 17:14:05,587] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:14:05,679] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:14:05,701] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 12.868 seconds
[2022-06-23 17:14:36,474] {processor.py:153} INFO - Started process (PID=2289) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:14:36,475] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:14:36,475] {logging_mixin.py:115} INFO - [2022-06-23 17:14:36,475] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:15:06,477] {logging_mixin.py:115} INFO - [2022-06-23 17:15:06,476] {timeout.py:67} ERROR - Process timed out, PID: 2289
[2022-06-23 17:15:06,480] {logging_mixin.py:115} INFO - [2022-06-23 17:15:06,478] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 2289
[2022-06-23 17:15:06,480] {logging_mixin.py:115} INFO - [2022-06-23 17:15:06,480] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:15:06,482] {logging_mixin.py:115} INFO - [2022-06-23 17:15:06,480] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 2289

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:15:06,483] {logging_mixin.py:115} INFO - [2022-06-23 17:15:06,482] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:15:06,485] {logging_mixin.py:115} INFO - [2022-06-23 17:15:06,483] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 76, in process_food_waste_data
    .parquet(f"gs://{BUCKET}/{raw_folder}/{parquet_file}")
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py", line 301, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o37.parquet
[2022-06-23 17:15:06,485] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:15:06,538] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.065 seconds
[2022-06-23 17:15:36,994] {processor.py:153} INFO - Started process (PID=2512) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:15:36,994] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:15:36,995] {logging_mixin.py:115} INFO - [2022-06-23 17:15:36,995] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:15:52,160] {logging_mixin.py:115} INFO - [2022-06-23 17:15:52,110] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:15:52,208] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:15:52,277] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 15.286 seconds
[2022-06-23 17:16:22,400] {processor.py:153} INFO - Started process (PID=2708) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:16:22,400] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:16:22,400] {logging_mixin.py:115} INFO - [2022-06-23 17:16:22,400] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:16:36,587] {logging_mixin.py:115} INFO - [2022-06-23 17:16:36,541] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 88, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 82, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o44.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (4c757ec115fa executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet. Column: [date_collected], Expected: timestamp, Found: INT32
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	... 20 more
[2022-06-23 17:16:36,631] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:16:36,654] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 14.258 seconds
[2022-06-23 17:17:06,752] {processor.py:153} INFO - Started process (PID=2907) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:17:06,753] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:17:06,755] {logging_mixin.py:115} INFO - [2022-06-23 17:17:06,754] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:18:32,188] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:18:32,207] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:18:32,209] {logging_mixin.py:115} INFO - [2022-06-23 17:18:32,209] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:19:02,220] {logging_mixin.py:115} INFO - [2022-06-23 17:19:02,219] {timeout.py:67} ERROR - Process timed out, PID: 46
[2022-06-23 17:19:02,221] {logging_mixin.py:115} INFO - [2022-06-23 17:19:02,220] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 46
[2022-06-23 17:19:02,221] {logging_mixin.py:115} INFO - [2022-06-23 17:19:02,221] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:19:02,222] {logging_mixin.py:115} INFO - [2022-06-23 17:19:02,221] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 46

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:19:02,223] {logging_mixin.py:115} INFO - [2022-06-23 17:19:02,223] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:19:02,223] {logging_mixin.py:115} INFO - [2022-06-23 17:19:02,223] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 45, in <module>
    .appName('process')\
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py", line 233, in getOrCreate
    session._jsparkSession.sessionState().conf().setConfString(key, value)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o23.sessionState
[2022-06-23 17:19:02,224] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:19:02,251] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.081 seconds
[2022-06-23 17:19:32,582] {processor.py:153} INFO - Started process (PID=246) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:19:32,583] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:19:32,584] {logging_mixin.py:115} INFO - [2022-06-23 17:19:32,583] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:20:02,587] {logging_mixin.py:115} INFO - [2022-06-23 17:20:02,585] {timeout.py:67} ERROR - Process timed out, PID: 246
[2022-06-23 17:20:02,591] {logging_mixin.py:115} INFO - [2022-06-23 17:20:02,588] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 246
[2022-06-23 17:20:02,592] {logging_mixin.py:115} INFO - [2022-06-23 17:20:02,592] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:20:02,596] {logging_mixin.py:115} INFO - [2022-06-23 17:20:02,593] {java_gateway.py:1056} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/timeout.py", line 68, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/02_data_process_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.3.2/best-practices.html#reducing-dag-complexity, PID: 246

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/clientserver.py", line 540, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2022-06-23 17:20:02,596] {logging_mixin.py:115} INFO - [2022-06-23 17:20:02,596] {clientserver.py:543} INFO - Closing down clientserver connection
[2022-06-23 17:20:02,617] {logging_mixin.py:115} INFO - [2022-06-23 17:20:02,611] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 87, in <module>
    process_food_waste_data(raw_folder='raw', parquet_file='brooklyn.parquet', cols_to_drop=['id', 'image_id'])
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 81, in process_food_waste_data
    print(df.head())
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1603, in head
    rs = self.head(1)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 1605, in head
    return self.take(n)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 744, in take
    return self.limit(num).collect()
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py", line 693, in collect
    sock_info = self._jdf.collectToPython()
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/airflow/spark/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o38.collectToPython
[2022-06-23 17:20:02,620] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:20:02,709] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 30.128 seconds
[2022-06-23 17:20:33,307] {processor.py:153} INFO - Started process (PID=468) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:20:33,308] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:20:33,309] {logging_mixin.py:115} INFO - [2022-06-23 17:20:33,309] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:20:46,620] {logging_mixin.py:115} INFO - Row(id='5e31d5503b85a2e63d634187', date_collected=datetime.date(2020, 1, 27), retailer_type='counter service', retailer_detail='ready-to-eat and prepackaged health food', food_type='ready-to-eat', food_detail='organic oatmeal', label_type='sticker', label_language='use by', label_date='2020-01-28', approximate_dollar_value=5.25, image_id='IMG_1872', collection_lat=40.69453, collection_long=-73.99447, label_explanation='')
[2022-06-23 17:20:46,633] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:20:46,692] {logging_mixin.py:115} INFO - [2022-06-23 17:20:46,691] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:20:46,726] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 13.422 seconds
[2022-06-23 17:21:16,807] {processor.py:153} INFO - Started process (PID=679) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:21:16,809] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:21:16,812] {logging_mixin.py:115} INFO - [2022-06-23 17:21:16,811] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:21:39,584] {logging_mixin.py:115} INFO - Row(id='5e31d5503b85a2e63d634187', date_collected=datetime.date(2020, 1, 27), retailer_type='counter service', retailer_detail='ready-to-eat and prepackaged health food', food_type='ready-to-eat', food_detail='organic oatmeal', label_type='sticker', label_language='use by', label_date='2020-01-28', approximate_dollar_value=5.25, image_id='IMG_1872', collection_lat=40.69453, collection_long=-73.99447, label_explanation='')
[2022-06-23 17:21:39,610] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:21:39,636] {logging_mixin.py:115} INFO - [2022-06-23 17:21:39,636] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:21:39,677] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 22.887 seconds
[2022-06-23 17:23:10,166] {processor.py:153} INFO - Started process (PID=47) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:23:10,167] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:23:10,168] {logging_mixin.py:115} INFO - [2022-06-23 17:23:10,168] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:23:30,655] {logging_mixin.py:115} INFO - [2022-06-23 17:23:30,650] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/registry.py", line 228, in get_filesystem_class
    register_implementation(protocol, _import_class(bit["class"]))
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/registry.py", line 251, in _import_class
    mod = importlib.import_module(mod)
  File "/usr/local/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 965, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'gcsfs'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 236, in read
    mode="rb",
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 84, in _get_path_or_handle
    path_or_handle, **(storage_options or {})
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/core.py", line 408, in url_to_fs
    cls = get_filesystem_class(protocol)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/registry.py", line 230, in get_filesystem_class
    raise ImportError(bit["err"]) from e
ImportError: Please install gcsfs to access Google Storage
[2022-06-23 17:23:30,657] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:23:30,744] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 20.582 seconds
[2022-06-23 17:24:00,909] {processor.py:153} INFO - Started process (PID=232) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:24:00,910] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:24:00,910] {logging_mixin.py:115} INFO - [2022-06-23 17:24:00,910] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:24:06,468] {logging_mixin.py:115} INFO - [2022-06-23 17:24:06,466] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/registry.py", line 228, in get_filesystem_class
    register_implementation(protocol, _import_class(bit["class"]))
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/registry.py", line 251, in _import_class
    mod = importlib.import_module(mod)
  File "/usr/local/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 965, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'gcsfs'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 236, in read
    mode="rb",
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 84, in _get_path_or_handle
    path_or_handle, **(storage_options or {})
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/core.py", line 408, in url_to_fs
    cls = get_filesystem_class(protocol)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/registry.py", line 230, in get_filesystem_class
    raise ImportError(bit["err"]) from e
ImportError: Please install gcsfs to access Google Storage
[2022-06-23 17:24:06,468] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:24:06,490] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 5.582 seconds
[2022-06-23 17:29:29,508] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:29:29,509] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:29:29,509] {logging_mixin.py:115} INFO - [2022-06-23 17:29:29,509] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:29:39,027] {logging_mixin.py:115} INFO - [2022-06-23 17:29:39,016] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:29:39,030] {logging_mixin.py:115} INFO - [2022-06-23 17:29:39,027] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:29:39,030] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:29:39,059] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 9.553 seconds
[2022-06-23 17:30:09,203] {processor.py:153} INFO - Started process (PID=224) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:30:09,204] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:30:09,208] {logging_mixin.py:115} INFO - [2022-06-23 17:30:09,208] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:30:22,974] {logging_mixin.py:115} INFO - [2022-06-23 17:30:22,973] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:30:22,975] {logging_mixin.py:115} INFO - [2022-06-23 17:30:22,974] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:30:22,976] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:30:22,995] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 13.809 seconds
[2022-06-23 17:30:53,580] {processor.py:153} INFO - Started process (PID=398) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:30:53,581] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:30:53,581] {logging_mixin.py:115} INFO - [2022-06-23 17:30:53,581] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:31:13,840] {logging_mixin.py:115} INFO - [2022-06-23 17:31:13,839] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:31:13,844] {logging_mixin.py:115} INFO - [2022-06-23 17:31:13,841] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:31:13,844] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:31:13,867] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 20.289 seconds
[2022-06-23 17:31:43,957] {processor.py:153} INFO - Started process (PID=591) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:31:43,958] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:31:43,959] {logging_mixin.py:115} INFO - [2022-06-23 17:31:43,959] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:31:47,679] {logging_mixin.py:115} INFO - [2022-06-23 17:31:47,678] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:31:47,683] {logging_mixin.py:115} INFO - [2022-06-23 17:31:47,680] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:31:47,684] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:31:47,718] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.766 seconds
[2022-06-23 17:32:18,121] {processor.py:153} INFO - Started process (PID=774) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:32:18,123] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:32:18,124] {logging_mixin.py:115} INFO - [2022-06-23 17:32:18,123] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:32:21,675] {logging_mixin.py:115} INFO - [2022-06-23 17:32:21,674] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:32:21,679] {logging_mixin.py:115} INFO - [2022-06-23 17:32:21,676] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:32:21,680] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:32:21,719] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.604 seconds
[2022-06-23 17:32:52,274] {processor.py:153} INFO - Started process (PID=950) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:32:52,276] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:32:52,277] {logging_mixin.py:115} INFO - [2022-06-23 17:32:52,276] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:32:56,051] {logging_mixin.py:115} INFO - [2022-06-23 17:32:56,050] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:32:56,055] {logging_mixin.py:115} INFO - [2022-06-23 17:32:56,052] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:32:56,056] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:32:56,083] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.814 seconds
[2022-06-23 17:33:26,437] {processor.py:153} INFO - Started process (PID=1122) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:33:26,437] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:33:26,438] {logging_mixin.py:115} INFO - [2022-06-23 17:33:26,438] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:33:30,298] {logging_mixin.py:115} INFO - [2022-06-23 17:33:30,297] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:33:30,302] {logging_mixin.py:115} INFO - [2022-06-23 17:33:30,299] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:33:30,303] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:33:30,330] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.895 seconds
[2022-06-23 17:34:00,609] {processor.py:153} INFO - Started process (PID=1295) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:34:00,610] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:34:00,610] {logging_mixin.py:115} INFO - [2022-06-23 17:34:00,610] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:34:11,040] {logging_mixin.py:115} INFO - [2022-06-23 17:34:11,039] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:34:11,045] {logging_mixin.py:115} INFO - [2022-06-23 17:34:11,041] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:34:11,045] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:34:11,074] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 10.467 seconds
[2022-06-23 17:34:41,828] {processor.py:153} INFO - Started process (PID=1483) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:34:41,830] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:34:41,830] {logging_mixin.py:115} INFO - [2022-06-23 17:34:41,830] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:34:45,546] {logging_mixin.py:115} INFO - [2022-06-23 17:34:45,545] {retry.py:151} ERROR - _request non-retriable exception: Invalid bucket name: '{BUCKET}', 400
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:34:45,550] {logging_mixin.py:115} INFO - [2022-06-23 17:34:45,547] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 91, in <module>
    df = pd.read_parquet("gs://{BUCKET}/raw/brooklyn.parquet")
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 500, in read_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/parquet.py", line 240, in read
    path_or_handle, columns=columns, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1915, in read_table
    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/parquet.py", line 1698, in __init__
    if filesystem.get_file_info(path_or_paths).is_file:
  File "pyarrow/_fs.pyx", line 439, in pyarrow._fs.FileSystem.get_file_info
  File "pyarrow/error.pxi", line 143, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/_fs.pyx", line 1101, in pyarrow._fs._cb_get_file_info
  File "/home/airflow/.local/lib/python3.7/site-packages/pyarrow/fs.py", line 307, in get_file_info
    info = self.fs.info(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 86, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 66, in sync
    raise return_result
  File "/home/airflow/.local/lib/python3.7/site-packages/fsspec/asyn.py", line 26, in _runner
    result[0] = await coro
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 717, in _info
    exact = await self._get_object(path)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 458, in _get_object
    res = await self._call("GET", "b/{}/o/{}", bucket, key, json_out=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 393, in _call
    method, path, *args, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/decorator.py", line 221, in fun
    return await caller(func, *(extras + args), **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 152, in retry_request
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 115, in retry_request
    return await func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/core.py", line 384, in _request
    validate_response(status, contents, path, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/gcsfs/retry.py", line 102, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Invalid bucket name: '{BUCKET}', 400
[2022-06-23 17:34:45,551] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:34:45,585] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.759 seconds
[2022-06-23 17:35:15,981] {processor.py:153} INFO - Started process (PID=1653) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:35:15,983] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:35:15,983] {logging_mixin.py:115} INFO - [2022-06-23 17:35:15,983] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:35:16,022] {logging_mixin.py:115} INFO - [2022-06-23 17:35:16,021] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 11, in <module>
    from scripts.process_food_waste_data import process_food_waste_data
  File "/opt/airflow/dags/scripts/process_food_waste_data.py", line 93
    gs://dtc-project-data_dtc-project-ritaafranco/raw/brooklyn.parquet
        ^
SyntaxError: invalid syntax
[2022-06-23 17:35:16,022] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:35:16,036] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.056 seconds
[2022-06-23 17:35:46,093] {processor.py:153} INFO - Started process (PID=1681) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:35:46,094] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:35:46,094] {logging_mixin.py:115} INFO - [2022-06-23 17:35:46,094] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:35:46,355] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:35:50,009] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:35:50,023] {logging_mixin.py:115} INFO - [2022-06-23 17:35:50,022] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:35:50,053] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.961 seconds
[2022-06-23 17:36:20,228] {processor.py:153} INFO - Started process (PID=1866) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:36:20,230] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:36:20,231] {logging_mixin.py:115} INFO - [2022-06-23 17:36:20,231] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:36:20,489] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:36:23,984] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:36:24,000] {logging_mixin.py:115} INFO - [2022-06-23 17:36:23,999] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:36:24,027] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.804 seconds
[2022-06-23 17:37:48,562] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:37:48,562] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:37:48,563] {logging_mixin.py:115} INFO - [2022-06-23 17:37:48,563] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:37:50,286] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:37:57,094] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:37:57,097] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:37:57,126] {logging_mixin.py:115} INFO - [2022-06-23 17:37:57,125] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:37:57,173] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.615 seconds
[2022-06-23 17:38:27,638] {processor.py:153} INFO - Started process (PID=237) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:38:27,640] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:38:27,641] {logging_mixin.py:115} INFO - [2022-06-23 17:38:27,641] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:38:28,208] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:38:44,129] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:38:44,131] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:38:44,153] {logging_mixin.py:115} INFO - [2022-06-23 17:38:44,152] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:38:44,191] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 16.556 seconds
[2022-06-23 17:39:14,909] {processor.py:153} INFO - Started process (PID=447) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:39:14,909] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:39:14,910] {logging_mixin.py:115} INFO - [2022-06-23 17:39:14,910] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:39:15,173] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:39:18,726] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:39:18,727] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:39:18,741] {logging_mixin.py:115} INFO - [2022-06-23 17:39:18,741] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:39:18,760] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.853 seconds
[2022-06-23 17:39:49,078] {processor.py:153} INFO - Started process (PID=630) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:39:49,080] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:39:49,080] {logging_mixin.py:115} INFO - [2022-06-23 17:39:49,080] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:39:49,349] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:39:53,479] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:39:53,481] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:39:53,496] {logging_mixin.py:115} INFO - [2022-06-23 17:39:53,496] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:39:53,521] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.447 seconds
[2022-06-23 17:40:10,320] {processor.py:153} INFO - Started process (PID=805) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:40:10,321] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:40:10,322] {logging_mixin.py:115} INFO - [2022-06-23 17:40:10,321] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:40:10,621] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:40:14,426] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:40:14,428] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:40:14,465] {logging_mixin.py:115} INFO - [2022-06-23 17:40:14,465] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:40:14,579] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.263 seconds
[2022-06-23 17:40:44,655] {processor.py:153} INFO - Started process (PID=987) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:40:44,655] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:40:44,656] {logging_mixin.py:115} INFO - [2022-06-23 17:40:44,656] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:40:45,192] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:41:04,928] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:41:04,934] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:41:04,997] {logging_mixin.py:115} INFO - [2022-06-23 17:41:04,996] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:41:05,095] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 20.443 seconds
[2022-06-23 17:41:36,037] {processor.py:153} INFO - Started process (PID=1195) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:41:36,038] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:41:36,039] {logging_mixin.py:115} INFO - [2022-06-23 17:41:36,039] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:41:36,323] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:41:40,113] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:41:40,115] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:41:40,129] {logging_mixin.py:115} INFO - [2022-06-23 17:41:40,129] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:41:40,150] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.118 seconds
[2022-06-23 17:42:10,228] {processor.py:153} INFO - Started process (PID=1379) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:42:10,229] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:42:10,229] {logging_mixin.py:115} INFO - [2022-06-23 17:42:10,229] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:42:10,527] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:42:14,314] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:42:14,316] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:42:14,330] {logging_mixin.py:115} INFO - [2022-06-23 17:42:14,329] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:42:14,350] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.127 seconds
[2022-06-23 17:42:44,416] {processor.py:153} INFO - Started process (PID=1563) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:42:44,417] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:42:44,417] {logging_mixin.py:115} INFO - [2022-06-23 17:42:44,417] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:42:44,706] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:42:48,439] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:42:48,440] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:42:48,455] {logging_mixin.py:115} INFO - [2022-06-23 17:42:48,455] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:42:48,477] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.062 seconds
[2022-06-23 17:43:18,563] {processor.py:153} INFO - Started process (PID=1755) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:43:18,563] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:43:18,564] {logging_mixin.py:115} INFO - [2022-06-23 17:43:18,564] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:43:18,839] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:43:22,605] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:43:22,607] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:43:22,622] {logging_mixin.py:115} INFO - [2022-06-23 17:43:22,621] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:43:22,652] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.091 seconds
[2022-06-23 17:43:52,761] {processor.py:153} INFO - Started process (PID=1939) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:43:52,763] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:43:52,763] {logging_mixin.py:115} INFO - [2022-06-23 17:43:52,763] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:43:53,034] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:43:57,800] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:43:57,801] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:43:57,817] {logging_mixin.py:115} INFO - [2022-06-23 17:43:57,816] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:43:57,839] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 5.083 seconds
[2022-06-23 17:44:28,018] {processor.py:153} INFO - Started process (PID=2126) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:44:28,018] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:44:28,018] {logging_mixin.py:115} INFO - [2022-06-23 17:44:28,018] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:44:28,288] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:44:32,824] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:44:32,826] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:44:32,840] {logging_mixin.py:115} INFO - [2022-06-23 17:44:32,840] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:44:32,863] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.847 seconds
[2022-06-23 17:45:03,200] {processor.py:153} INFO - Started process (PID=2311) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:45:03,202] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:45:03,203] {logging_mixin.py:115} INFO - [2022-06-23 17:45:03,203] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:45:03,459] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:45:07,957] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:45:07,959] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:45:07,972] {logging_mixin.py:115} INFO - [2022-06-23 17:45:07,971] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:45:07,993] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.795 seconds
[2022-06-23 17:45:38,354] {processor.py:153} INFO - Started process (PID=2502) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:45:38,355] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:45:38,355] {logging_mixin.py:115} INFO - [2022-06-23 17:45:38,355] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:45:38,653] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:45:42,194] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:45:42,196] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:45:42,209] {logging_mixin.py:115} INFO - [2022-06-23 17:45:42,209] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:45:42,232] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.880 seconds
[2022-06-23 17:46:12,540] {processor.py:153} INFO - Started process (PID=2688) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:46:12,542] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:46:12,543] {logging_mixin.py:115} INFO - [2022-06-23 17:46:12,543] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:46:12,799] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:46:16,493] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:46:16,494] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:46:16,507] {logging_mixin.py:115} INFO - [2022-06-23 17:46:16,507] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:46:16,519] {logging_mixin.py:115} INFO - [2022-06-23 17:46:16,519] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:46:16,535] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.000 seconds
[2022-06-23 17:46:46,623] {processor.py:153} INFO - Started process (PID=2873) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:46:46,625] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:46:46,626] {logging_mixin.py:115} INFO - [2022-06-23 17:46:46,626] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:46:46,890] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:46:50,351] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:46:50,352] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:46:50,366] {logging_mixin.py:115} INFO - [2022-06-23 17:46:50,365] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:46:50,378] {logging_mixin.py:115} INFO - [2022-06-23 17:46:50,378] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:46:50,386] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.768 seconds
[2022-06-23 17:47:20,787] {processor.py:153} INFO - Started process (PID=3056) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:47:20,787] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:47:20,788] {logging_mixin.py:115} INFO - [2022-06-23 17:47:20,787] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:47:21,088] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:47:24,872] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:47:24,873] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:47:24,887] {logging_mixin.py:115} INFO - [2022-06-23 17:47:24,887] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:47:24,900] {logging_mixin.py:115} INFO - [2022-06-23 17:47:24,900] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:47:24,908] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.123 seconds
[2022-06-23 17:47:55,024] {processor.py:153} INFO - Started process (PID=3239) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:47:55,026] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:47:55,026] {logging_mixin.py:115} INFO - [2022-06-23 17:47:55,026] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:47:55,339] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:47:59,975] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:47:59,976] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:47:59,990] {logging_mixin.py:115} INFO - [2022-06-23 17:47:59,989] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:48:00,001] {logging_mixin.py:115} INFO - [2022-06-23 17:48:00,001] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:48:00,010] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.991 seconds
[2022-06-23 17:48:30,218] {processor.py:153} INFO - Started process (PID=3424) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:48:30,220] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:48:30,221] {logging_mixin.py:115} INFO - [2022-06-23 17:48:30,221] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:48:30,491] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:48:34,998] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:48:34,999] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:48:35,013] {logging_mixin.py:115} INFO - [2022-06-23 17:48:35,012] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:48:35,024] {logging_mixin.py:115} INFO - [2022-06-23 17:48:35,024] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:48:35,032] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.820 seconds
[2022-06-23 17:49:05,385] {processor.py:153} INFO - Started process (PID=3618) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:49:05,387] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:49:05,388] {logging_mixin.py:115} INFO - [2022-06-23 17:49:05,388] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:49:05,649] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:49:09,256] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:49:09,257] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:49:09,271] {logging_mixin.py:115} INFO - [2022-06-23 17:49:09,271] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:49:09,283] {logging_mixin.py:115} INFO - [2022-06-23 17:49:09,283] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:49:09,291] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.910 seconds
[2022-06-23 17:49:39,546] {processor.py:153} INFO - Started process (PID=3802) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:49:39,556] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:49:39,556] {logging_mixin.py:115} INFO - [2022-06-23 17:49:39,556] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:49:39,826] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:49:43,771] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:49:43,772] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:49:43,786] {logging_mixin.py:115} INFO - [2022-06-23 17:49:43,785] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:49:43,798] {logging_mixin.py:115} INFO - [2022-06-23 17:49:43,798] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:49:43,809] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.265 seconds
[2022-06-23 17:50:14,732] {processor.py:153} INFO - Started process (PID=3985) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:50:14,742] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:50:14,743] {logging_mixin.py:115} INFO - [2022-06-23 17:50:14,742] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:50:14,995] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:50:18,843] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:50:18,845] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:50:18,860] {logging_mixin.py:115} INFO - [2022-06-23 17:50:18,859] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:50:18,873] {logging_mixin.py:115} INFO - [2022-06-23 17:50:18,873] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:50:18,885] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.158 seconds
[2022-06-23 17:50:42,970] {processor.py:153} INFO - Started process (PID=4171) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:50:42,971] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:50:42,972] {logging_mixin.py:115} INFO - [2022-06-23 17:50:42,971] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:50:42,979] {logging_mixin.py:115} INFO - [2022-06-23 17:50:42,979] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator, PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.providers.apache.spark.operators.spark_submit' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py)
[2022-06-23 17:50:42,980] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:50:42,994] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.029 seconds
[2022-06-23 17:51:27,214] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:51:27,216] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:51:27,217] {logging_mixin.py:115} INFO - [2022-06-23 17:51:27,217] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:51:27,227] {logging_mixin.py:115} INFO - [2022-06-23 17:51:27,226] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator, PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.providers.apache.spark.operators.spark_submit' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py)
[2022-06-23 17:51:27,227] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:51:27,250] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.038 seconds
[2022-06-23 17:51:57,333] {processor.py:153} INFO - Started process (PID=68) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:51:57,333] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:51:57,334] {logging_mixin.py:115} INFO - [2022-06-23 17:51:57,334] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:51:57,342] {logging_mixin.py:115} INFO - [2022-06-23 17:51:57,342] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator, PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.providers.apache.spark.operators.spark_submit' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py)
[2022-06-23 17:51:57,342] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:51:57,356] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.029 seconds
[2022-06-23 17:52:27,451] {processor.py:153} INFO - Started process (PID=98) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:52:27,452] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:52:27,452] {logging_mixin.py:115} INFO - [2022-06-23 17:52:27,452] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:52:27,455] {logging_mixin.py:115} INFO - [2022-06-23 17:52:27,455] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator, PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.providers.apache.spark.operators.spark_submit' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py)
[2022-06-23 17:52:27,455] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:52:27,470] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.021 seconds
[2022-06-23 17:52:57,556] {processor.py:153} INFO - Started process (PID=129) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:52:57,557] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:52:57,557] {logging_mixin.py:115} INFO - [2022-06-23 17:52:57,557] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:52:57,565] {logging_mixin.py:115} INFO - [2022-06-23 17:52:57,564] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator, PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.providers.apache.spark.operators.spark_submit' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py)
[2022-06-23 17:52:57,565] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:52:57,578] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.028 seconds
[2022-06-23 17:53:27,663] {processor.py:153} INFO - Started process (PID=151) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:53:27,664] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:53:27,664] {logging_mixin.py:115} INFO - [2022-06-23 17:53:27,664] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:53:27,667] {logging_mixin.py:115} INFO - [2022-06-23 17:53:27,667] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator, PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.providers.apache.spark.operators.spark_submit' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py)
[2022-06-23 17:53:27,667] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:53:27,684] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.023 seconds
[2022-06-23 17:53:57,764] {processor.py:153} INFO - Started process (PID=181) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:53:57,765] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:53:57,765] {logging_mixin.py:115} INFO - [2022-06-23 17:53:57,765] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:53:57,771] {logging_mixin.py:115} INFO - [2022-06-23 17:53:57,771] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/02_data_process_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/02_data_process_dag.py", line 7, in <module>
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator, PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.providers.apache.spark.operators.spark_submit' (/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py)
[2022-06-23 17:53:57,771] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:53:57,786] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 0.024 seconds
[2022-06-23 17:54:27,864] {processor.py:153} INFO - Started process (PID=211) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:54:27,864] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:54:27,865] {logging_mixin.py:115} INFO - [2022-06-23 17:54:27,865] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:54:28,536] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:54:33,313] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:54:33,315] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:54:33,350] {logging_mixin.py:115} INFO - [2022-06-23 17:54:33,350] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:54:33,446] {logging_mixin.py:115} INFO - [2022-06-23 17:54:33,446] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:54:33,459] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 5.597 seconds
[2022-06-23 17:55:12,020] {processor.py:153} INFO - Started process (PID=45) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:55:12,021] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:55:12,022] {logging_mixin.py:115} INFO - [2022-06-23 17:55:12,022] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:55:13,868] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:55:20,448] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:55:20,451] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:55:20,490] {logging_mixin.py:115} INFO - [2022-06-23 17:55:20,490] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:55:20,526] {logging_mixin.py:115} INFO - [2022-06-23 17:55:20,526] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:55:20,547] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.530 seconds
[2022-06-23 17:55:50,623] {processor.py:153} INFO - Started process (PID=236) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:55:50,625] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:55:50,625] {logging_mixin.py:115} INFO - [2022-06-23 17:55:50,625] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:55:50,887] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:55:54,313] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:55:54,314] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:55:54,327] {logging_mixin.py:115} INFO - [2022-06-23 17:55:54,327] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:55:54,340] {logging_mixin.py:115} INFO - [2022-06-23 17:55:54,340] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:55:54,347] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.726 seconds
[2022-06-23 17:56:24,758] {processor.py:153} INFO - Started process (PID=418) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:56:24,760] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:56:24,760] {logging_mixin.py:115} INFO - [2022-06-23 17:56:24,760] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:56:25,029] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:56:29,050] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:56:29,051] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:56:29,064] {logging_mixin.py:115} INFO - [2022-06-23 17:56:29,064] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:56:29,076] {logging_mixin.py:115} INFO - [2022-06-23 17:56:29,076] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:56:29,085] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.331 seconds
[2022-06-23 17:56:59,927] {processor.py:153} INFO - Started process (PID=603) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:56:59,928] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:56:59,929] {logging_mixin.py:115} INFO - [2022-06-23 17:56:59,929] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:57:00,196] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:57:03,928] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:57:03,930] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:57:03,945] {logging_mixin.py:115} INFO - [2022-06-23 17:57:03,945] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:57:03,959] {logging_mixin.py:115} INFO - [2022-06-23 17:57:03,959] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:57:03,968] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.046 seconds
[2022-06-23 17:58:00,097] {processor.py:153} INFO - Started process (PID=37) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:58:00,098] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:58:00,099] {logging_mixin.py:115} INFO - [2022-06-23 17:58:00,099] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:58:01,364] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:58:08,128] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:58:08,130] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:58:08,159] {logging_mixin.py:115} INFO - [2022-06-23 17:58:08,158] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:58:08,189] {logging_mixin.py:115} INFO - [2022-06-23 17:58:08,189] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:58:08,212] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 8.118 seconds
[2022-06-23 17:58:38,287] {processor.py:153} INFO - Started process (PID=235) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:58:38,288] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:58:38,289] {logging_mixin.py:115} INFO - [2022-06-23 17:58:38,288] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:58:38,555] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:58:42,024] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:58:42,025] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:58:42,039] {logging_mixin.py:115} INFO - [2022-06-23 17:58:42,039] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:58:42,051] {logging_mixin.py:115} INFO - [2022-06-23 17:58:42,051] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:58:42,058] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.773 seconds
[2022-06-23 17:59:12,377] {processor.py:153} INFO - Started process (PID=422) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:59:12,378] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:59:12,378] {logging_mixin.py:115} INFO - [2022-06-23 17:59:12,378] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:59:12,636] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:59:16,209] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:59:16,210] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:59:16,222] {logging_mixin.py:115} INFO - [2022-06-23 17:59:16,222] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:59:16,234] {logging_mixin.py:115} INFO - [2022-06-23 17:59:16,234] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:59:16,243] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.871 seconds
[2022-06-23 17:59:46,529] {processor.py:153} INFO - Started process (PID=604) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:59:46,529] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 17:59:46,530] {logging_mixin.py:115} INFO - [2022-06-23 17:59:46,530] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:59:46,787] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 17:59:50,505] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 17:59:50,507] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 17:59:50,520] {logging_mixin.py:115} INFO - [2022-06-23 17:59:50,520] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 17:59:50,532] {logging_mixin.py:115} INFO - [2022-06-23 17:59:50,532] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 17:59:50,540] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.013 seconds
[2022-06-23 18:00:20,614] {processor.py:153} INFO - Started process (PID=788) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:00:20,623] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:00:20,624] {logging_mixin.py:115} INFO - [2022-06-23 18:00:20,624] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:00:20,891] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:00:24,698] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:00:24,699] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:00:24,712] {logging_mixin.py:115} INFO - [2022-06-23 18:00:24,712] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:00:24,725] {logging_mixin.py:115} INFO - [2022-06-23 18:00:24,725] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:00:24,732] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.120 seconds
[2022-06-23 18:00:54,813] {processor.py:153} INFO - Started process (PID=972) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:00:54,822] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:00:54,823] {logging_mixin.py:115} INFO - [2022-06-23 18:00:54,823] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:00:55,083] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:00:59,496] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:00:59,497] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:00:59,511] {logging_mixin.py:115} INFO - [2022-06-23 18:00:59,511] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:00:59,523] {logging_mixin.py:115} INFO - [2022-06-23 18:00:59,523] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:00:59,532] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.725 seconds
[2022-06-23 18:01:29,938] {processor.py:153} INFO - Started process (PID=1164) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:01:29,940] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:01:29,940] {logging_mixin.py:115} INFO - [2022-06-23 18:01:29,940] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:01:30,201] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:01:33,741] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:01:33,742] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:01:33,755] {logging_mixin.py:115} INFO - [2022-06-23 18:01:33,755] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:01:33,767] {logging_mixin.py:115} INFO - [2022-06-23 18:01:33,767] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:01:33,774] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.839 seconds
[2022-06-23 18:02:04,184] {processor.py:153} INFO - Started process (PID=1348) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:02:04,185] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:02:04,185] {logging_mixin.py:115} INFO - [2022-06-23 18:02:04,185] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:02:04,470] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:02:08,183] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:02:08,184] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:02:08,198] {logging_mixin.py:115} INFO - [2022-06-23 18:02:08,198] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:02:08,210] {logging_mixin.py:115} INFO - [2022-06-23 18:02:08,210] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:02:08,217] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.036 seconds
[2022-06-23 18:02:38,279] {processor.py:153} INFO - Started process (PID=1534) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:02:38,280] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:02:38,280] {logging_mixin.py:115} INFO - [2022-06-23 18:02:38,280] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:02:38,543] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:02:41,953] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:02:41,954] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:02:41,968] {logging_mixin.py:115} INFO - [2022-06-23 18:02:41,968] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:02:41,980] {logging_mixin.py:115} INFO - [2022-06-23 18:02:41,980] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:02:41,989] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.712 seconds
[2022-06-23 18:03:12,442] {processor.py:153} INFO - Started process (PID=1718) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:03:12,443] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:03:12,443] {logging_mixin.py:115} INFO - [2022-06-23 18:03:12,443] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:03:12,698] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:03:16,220] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:03:16,221] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:03:16,234] {logging_mixin.py:115} INFO - [2022-06-23 18:03:16,233] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:03:16,245] {logging_mixin.py:115} INFO - [2022-06-23 18:03:16,245] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:03:16,254] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.815 seconds
[2022-06-23 18:03:46,606] {processor.py:153} INFO - Started process (PID=1903) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:03:46,606] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:03:46,606] {logging_mixin.py:115} INFO - [2022-06-23 18:03:46,606] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:03:46,863] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:03:50,292] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:03:50,294] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:03:50,306] {logging_mixin.py:115} INFO - [2022-06-23 18:03:50,306] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:03:50,318] {logging_mixin.py:115} INFO - [2022-06-23 18:03:50,318] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:03:50,325] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 3.721 seconds
[2022-06-23 18:04:20,819] {processor.py:153} INFO - Started process (PID=2086) to work on /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:04:20,820] {processor.py:641} INFO - Processing file /opt/airflow/dags/02_data_process_dag.py for tasks to queue
[2022-06-23 18:04:20,821] {logging_mixin.py:115} INFO - [2022-06-23 18:04:20,821] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:04:21,076] {logging_mixin.py:115} INFO - ######## dtc-project-data_dtc-project-ritaafranco
[2022-06-23 18:04:25,350] {logging_mixin.py:115} INFO -                          id date_collected  ... collection_long label_explanation
0  5e31d5503b85a2e63d634187     2020-01-27  ...       -73.99447                  
1  5e31d5643b85a2e63d634188     2020-01-27  ...       -73.99447                  
2  5e31d8903b85a2e63d634189     2020-01-27  ...       -73.99447                  
3  5e31dcee3b85a2e63d63418a     2020-01-27  ...       -73.99447                  
4  5e31dd273b85a2e63d63418b     2020-01-27  ...       -73.99447                  

[5 rows x 14 columns]
[2022-06-23 18:04:25,351] {processor.py:651} INFO - DAG(s) dict_keys(['process-food-waste-data']) retrieved from /opt/airflow/dags/02_data_process_dag.py
[2022-06-23 18:04:25,364] {logging_mixin.py:115} INFO - [2022-06-23 18:04:25,364] {dag.py:2379} INFO - Sync 1 DAGs
[2022-06-23 18:04:25,376] {logging_mixin.py:115} INFO - [2022-06-23 18:04:25,376] {dag.py:2931} INFO - Setting next_dagrun for process-food-waste-data to None, run_after=None
[2022-06-23 18:04:25,383] {processor.py:161} INFO - Processing /opt/airflow/dags/02_data_process_dag.py took 4.566 seconds
